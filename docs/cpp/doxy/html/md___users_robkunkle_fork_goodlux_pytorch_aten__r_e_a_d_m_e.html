<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>My Project: ATen: A TENsor library</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">My Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">ATen: A TENsor library </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>ATen is a simple tensor library thats exposes the Tensor operations in Torch and PyTorch directly in C++11. The wrapper respects the semantics of operators in PyTorch, except minor details due to differences between C++ and Python in the way default arguments are handled. See the <a href="http://pytorch.org/docs/tensors.html">documentation for tensors</a> in PyTorch for what these operations do. ATen's API is auto-generated from the same declarations PyTorch uses so the two APIs will track each other over time.</p>
<p>Tensor types are resolved dynamically, such that the API is generic and does not include templates. That is, there is one <code>Tensor</code> type. It can hold a CPU or CUDA Tensor, and the tensor may have Doubles, Float, Ints, etc. This design makes it easy to write generic code without templating everything.</p>
<p>See the <em>generated</em> <a href="doc/Tensor.h"><code>Tensor.h</code> file</a> and <a href="doc/Functions.h"><code>Functions.h</code> file</a> for the provided API. Excerpt: </p><div class="fragment"><div class="line">{c++}</div><div class="line">Tensor atan2(const Tensor &amp; other) const;</div><div class="line">Tensor &amp; atan2_(const Tensor &amp; other);</div><div class="line">Tensor pow(Scalar exponent) const;</div><div class="line">Tensor pow(const Tensor &amp; exponent) const;</div><div class="line">Tensor &amp; pow_(Scalar exponent);</div><div class="line">Tensor &amp; pow_(const Tensor &amp; exponent);</div><div class="line">Tensor lerp(const Tensor &amp; end, Scalar weight) const;</div><div class="line">Tensor &amp; lerp_(const Tensor &amp; end, Scalar weight);</div><div class="line">Tensor histc() const;</div><div class="line">Tensor histc(int64_t bins) const;</div><div class="line">Tensor histc(int64_t bins, Scalar min) const;</div><div class="line">Tensor histc(int64_t bins, Scalar min, Scalar max) const;</div></div><!-- fragment --><p>Inplace operations are also provided, and always suffixed by <code>_</code> to indicate they will modify the Tensor.</p>
<h3>Installation</h3>
<p>TH/THC/THNN/THCUNN are provided (as git subtrees), so the repo is standalone. You will need a C++11 compiler, cmake, and the pyyaml python package. </p><div class="fragment"><div class="line"># Install pyyaml used by python code generation to read API declarations</div><div class="line"></div><div class="line"># macOS: if you don&#39;t have pip</div><div class="line">sudo easy_install pip</div><div class="line"># Ubuntu: if you don&#39;t have pip</div><div class="line">apt-get -y install python-pip</div><div class="line"></div><div class="line"># if you don&#39;t have pyyaml</div><div class="line">sudo pip install pyyaml</div><div class="line"></div><div class="line">mkdir build</div><div class="line">cd build</div><div class="line">cmake .. -DCMAKE_INSTALL_PREFIX=/where/you/want # specify your dest directory</div><div class="line"># cmake .. -DUSE_NVRTC=ON -DUSE_TENSORRT=OFF -DCMAKE_INSTALL_PREFIX=../install -DCAFFE2_CMAKE_BUILDING_WITH_MAIN_REPO=OFF -DUSE_CUDA=ON # for CUDA</div><div class="line"># cmake .. -DUSE_CUDA=OFF  # for CPU only machines</div><div class="line">make install</div></div><!-- fragment --><h3>Example usage</h3>
<p>Here is a simple example; again, the syntax follows Torch semantics.</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">using namespace at; // assumed in the following</div><div class="line"></div><div class="line">Tensor d = CPU(kFloat).ones({3, 4});</div><div class="line">Tensor r = CPU(kFloat).zeros({3,4});</div><div class="line">for(auto i = 0; i &lt; 100000; i++) {</div><div class="line">  r = r.add(d);</div><div class="line">  // equivalently</div><div class="line">  r = r + d;</div><div class="line">  // or</div><div class="line">  r += d;</div><div class="line">}</div></div><!-- fragment --><p>Want this running on the GPU? </p><div class="fragment"><div class="line">{c++}</div><div class="line">using namespace at; // assumed in the following</div><div class="line"></div><div class="line">Tensor d = CUDA(kFloat).ones({3, 4});</div><div class="line">Tensor r = CUDA(kFloat).zeros({3,4});</div><div class="line">for(auto i = 0; i &lt; 100000; i++) {</div><div class="line">  r = r.add(d);</div><div class="line">  // equivalently</div><div class="line">  r = r + d;</div><div class="line">  // or</div><div class="line">  r += d;</div><div class="line">}</div></div><!-- fragment --><p>Expressions like <code>CUDA(kFloat)</code> are first-class <code>at::Type</code> objects that represent the type of a Tensor and are used to create Tensors when their type cannot be inferred. See the <em>generated</em> <a href="doc/Type.h">Type header</a> for its API.</p>
<p>See more in <a href="src/ATen/test">sample files</a>.</p>
<h3>Creating your kernel</h3>
<p>It is easy to create new kernels, thanks to the <code>dispatch&lt;&gt;()</code> templated function. Example: </p><div class="fragment"><div class="line">{c++}</div><div class="line"></div><div class="line">// a simple sum kernel (for CPU only)</div><div class="line">template&lt;typename T&gt;</div><div class="line">struct sum_op {</div><div class="line">  // dispatch handles variable arguments for you</div><div class="line">  Tensor CPU(const Type &amp; t, Tensor &amp; x_)</div><div class="line">  {</div><div class="line">    Tensor x = x_.contiguous();</div><div class="line">    auto x_p = x.data&lt;T&gt;();</div><div class="line">    int64_t size = x.numel();</div><div class="line">    T sum = 0;</div><div class="line">    for(int64_t i = 0; i &lt; size; i++) {</div><div class="line">      sum += x_p[i];</div><div class="line">    }</div><div class="line">    return sum;</div><div class="line">  };</div><div class="line">  Tensor CUDA(Tensor&amp; x) {</div><div class="line">    throw std::invalid_argument(&quot;device not supported&quot;);</div><div class="line">  };</div><div class="line">};</div><div class="line"></div><div class="line">Tensor a = CPU(kFloat).rand({3, 7});</div><div class="line">std::cout &lt;&lt; a &lt;&lt; std::endl;</div><div class="line">std::cout &lt;&lt; dispatch&lt;sum_op&gt;(a.type(),a) &lt;&lt; &quot; == &quot; &lt;&lt; a.sum() &lt;&lt; std::endl;</div></div><!-- fragment --><h3>Efficient access to tensor elements</h3>
<p>When using Tensor-wide operations, the relative cost of dynamic dispatch is very small. However, there are cases, especially in your own kernels, where efficient element-wise access is needed, and the cost of dynamic dispatch inside the element-wise loop is very high. ATen provides <em>accessors</em> that are created with a single dynamic check that a Tensor is the type and number of dimensions. Accessors then expose an API for accessing the Tensor elements efficiently:</p>
<div class="fragment"><div class="line">{c++}</div><div class="line"></div><div class="line">Tensor foo = CPU(kFloat).rand({12,12});</div><div class="line"></div><div class="line">// assert foo is 2-dimensional and holds floats.</div><div class="line">auto foo_a = foo.accessor&lt;float,2&gt;();</div><div class="line">float trace = 0;</div><div class="line"></div><div class="line">for(int i = 0; i &lt; foo_a.size(0); i++) {</div><div class="line">  // use the accessor foo_a to get tensor data.</div><div class="line">  trace += foo_a[i][i];</div><div class="line">}</div></div><!-- fragment --><p>Accessors are temporary views of a Tensor. They are only valid for the lifetime of the tensor that they view and hence should only be used locally in a function, like iterators.</p>
<h3>Using externally created data</h3>
<p>If you already have your tensor data allocated in memory (CPU or CUDA), you can view that memory as a Tensor in ATen:</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">float data[] = { 1, 2, 3,</div><div class="line">                 4, 5, 6};</div><div class="line">auto f = CPU(kFloat).tensorFromBlob(data, {2,3});</div><div class="line">cout &lt;&lt; f &lt;&lt; endl;</div></div><!-- fragment --><p>These tensors cannot be resized because ATen does not own the memory, but otherwise behave as normal tensors.</p>
<h3>Scalars and zero-dimensional tensors</h3>
<p>In addition to the <code>Tensor</code> objects, ATen also includes <code>Scalar</code>s that represent a single number. Like a Tensor, Scalars are dynamically typed and can hold any one of ATen's <a href="doc/Type.h">number types</a>. Scalars can be implicitly constructed from C++ number types. Scalars are needed because some functions like <code>addmm</code> take numbers along with Tensors and expect these numbers to be the same dynamic type as the tensor. They are also used in the API to indicate places where a function will <em>always</em> return a Scalar value, like <code>sum</code>.</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">Tensor addmm(Scalar beta, const Tensor &amp; self,</div><div class="line">             Scalar alpha, const Tensor &amp; mat1,</div><div class="line">             const Tensor &amp; mat2);</div><div class="line">Scalar sum(const Tensor &amp; self);</div><div class="line"></div><div class="line">//usage</div><div class="line">Tensor a = ...</div><div class="line">Tensor b = ...</div><div class="line">Tensor c = ...</div><div class="line">Tensor r = addmm(1.0, a, .5, b, c);</div></div><!-- fragment --><p>In addition to Scalars, ATen also allows Tensor objects to be zero-dimensional. These Tensors hold a single value and they can be references to a single element in a larger Tensor. They can be used anywhere a Tensor is expected. They are normally created by operators like <code>select</code> which reduce the dimensions of a Tensor.</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">Tensor two = CPU(kFloat).rand({10,20});</div><div class="line">two[1][2] = 4;</div><div class="line">//~~~~~~~  zero-dimensional Tensor</div></div><!-- fragment --><p>It is possible to convert between Scalar and zero-dim Tensors:</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">Tensor zero_dim = CPU(kFloat).scalarTensor(4);</div><div class="line">Scalar from_tensor = Scalar(zero_dim); //only valid when zero_dim.dim() == 0;</div></div><!-- fragment --><h3>Avoiding unnecessary CUDA synchronization in your kernels when using Scalars</h3>
<p>Moving a single number from the GPU to the CPU introduces a synchronization point that can add latency to your program. In certain cases the result of a GPU operator like <code>sum</code> which returns a Scalar may be plugged into another GPU operator as an argument. If Scalars were always copied to the CPU, this would result in 2 copies. To avoid these synchronizations, Scalar objects can be optionally backed by a zero-dim Tensor, and are only copied to the CPU when requested.</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">auto a = CUDA(kFloat).rand({3,4});</div><div class="line">Scalar on_gpu = Scalar(a[1][1]); //backed by zero-dim Tensor</div><div class="line">assert(on_gpu.isBackedByTensor());</div><div class="line"></div><div class="line">double value = on_gpu.toDouble(); // copied to CPU, if it was backed by GPU Tensor.</div><div class="line">Scalar svalue = on_gpu.local(); // force the Scalar to become local to CPU.</div><div class="line"></div><div class="line">// get the scalar as a zero-dim tensor. If it was already backed</div><div class="line">// by a zero-dim Tensor then this op has no synchronization.</div><div class="line">// if the Scalar was local on CPU, it performs the copy</div><div class="line">Tensor same_tensor = CUDA(kFloat).scalarTensor(on_gpu);</div></div><!-- fragment --><p>Operators aware of the location of Scalars can arrange to do the minimal number of copies required.</p>
<h3>Developer notes</h3>
<p>ATen relies heavily on code generation to automatically generate headers and implementations for all of the tensor methods it supports. The main entry point for the script which does all this work is <a href="src/ATen/gen.py"><code>src/ATen/gen.py</code></a>, which ingests <a href="src/ATen/Declarations.cwrap"><code>src/ATen/Declarations.cwrap</code></a>, <a href="src/ATen/nn.yaml"><code>src/ATen/nn.yaml</code></a>, <a href="src/ATen/native/native_functions.yaml"><code>src/ATen/native/native_functions.yaml</code></a> and the THNN/THCUNN headers and produces all of the headers and wrapping code necessary to generate the ATen interface.</p>
<p>If you need to understand how ATen understands a declaration after all of this processing occurs, it's helpful to look at the generated file <code>Declarations.yaml</code> (NB: not cwrap) which contains information for all ATen methods in a uniform manner. This file is utilized by PyTorch which further extends the ATen interface with support for automatic differentation.</p>
<h4>Note [ATen preprocessor philosophy]</h4>
<p>ATen is designed to be simple to use, and one of the things this implies is that it should not be necessary to use preprocessor macros when using ATen; we would rather provide all symbols, even for functionality that is not available on the system ATen is running on.</p>
<p>This means that internally inside ATen, whereas other libraries might simply omit source files for, e.g., CuDNN, when CuDNN libraries are not installed, ATen will always build these source files, compiling stub functions for anything that is not available. ATen never uses <code>AT_ENABLED_CUDA()</code> in header files, and all types in ATen's public API are always available no matter your build configuration. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.14
</small></address>
</body>
</html>
