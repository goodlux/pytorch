<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.14">
  <compounddef id="_reduce_ops_8cpp" kind="file" language="C++">
    <compoundname>ReduceOps.cpp</compoundname>
    <includes refid="_a_ten_8h" local="yes">ATen/ATen.h</includes>
    <includes refid="_dispatch_8h" local="yes">ATen/Dispatch.h</includes>
    <includes refid="_expand_utils_8h" local="yes">ATen/ExpandUtils.h</includes>
    <includes local="yes">ATen/NativeFunctions.h</includes>
    <includes refid="_wrap_dim_utils_8h" local="yes">ATen/WrapDimUtils.h</includes>
    <includes refid="_wrap_dim_utils_multi_8h" local="yes">ATen/WrapDimUtilsMulti.h</includes>
    <includes refid="_reduce_ops_utils_8h" local="yes">ReduceOpsUtils.h</includes>
    <includes refid="_reduce_ops_kernel_8h" local="yes">cpu/ReduceOpsKernel.h</includes>
    <includes local="no">algorithm</includes>
    <includes local="no">functional</includes>
    <includes local="no">limits</includes>
    <includes local="no">numeric</includes>
    <includes local="no">vector</includes>
    <includes local="no">map</includes>
    <incdepgraph>
      <node id="7152">
        <label>ATen/optional.h</label>
        <link refid="optional_8h_source"/>
        <childnode refid="7153" relation="include">
        </childnode>
        <childnode refid="7154" relation="include">
        </childnode>
        <childnode refid="7155" relation="include">
        </childnode>
        <childnode refid="7156" relation="include">
        </childnode>
        <childnode refid="7157" relation="include">
        </childnode>
        <childnode refid="7158" relation="include">
        </childnode>
        <childnode refid="7159" relation="include">
        </childnode>
      </node>
      <node id="7198">
        <label>ATen/detail/CUDAHooksInterface.h</label>
        <link refid="_c_u_d_a_hooks_interface_8h_source"/>
        <childnode refid="7148" relation="include">
        </childnode>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7196" relation="include">
        </childnode>
        <childnode refid="7199" relation="include">
        </childnode>
        <childnode refid="7160" relation="include">
        </childnode>
        <childnode refid="7157" relation="include">
        </childnode>
        <childnode refid="7149" relation="include">
        </childnode>
      </node>
      <node id="7166">
        <label>ATen/Device.h</label>
        <link refid="_device_8h_source"/>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7160" relation="include">
        </childnode>
        <childnode refid="7182" relation="include">
        </childnode>
        <childnode refid="7158" relation="include">
        </childnode>
        <childnode refid="7157" relation="include">
        </childnode>
      </node>
      <node id="7165">
        <label>atomic</label>
      </node>
      <node id="7157">
        <label>functional</label>
      </node>
      <node id="7203">
        <label>ATen/Backtrace.h</label>
        <link refid="_backtrace_8h_source"/>
        <childnode refid="7160" relation="include">
        </childnode>
        <childnode refid="7158" relation="include">
        </childnode>
        <childnode refid="7193" relation="include">
        </childnode>
        <childnode refid="7146" relation="include">
        </childnode>
      </node>
      <node id="7167">
        <label>ATen/ScalarType.h</label>
        <link refid="_scalar_type_8h_source"/>
        <childnode refid="7168" relation="include">
        </childnode>
        <childnode refid="7146" relation="include">
        </childnode>
        <childnode refid="7178" relation="include">
        </childnode>
        <childnode refid="7180" relation="include">
        </childnode>
        <childnode refid="7184" relation="include">
        </childnode>
      </node>
      <node id="7179">
        <label>limits</label>
      </node>
      <node id="7187">
        <label>assert.h</label>
      </node>
      <node id="7192">
        <label>ATen/Utils.h</label>
        <link refid="aten_2src_2_a_ten_2utils_8h_source"/>
        <childnode refid="7146" relation="include">
        </childnode>
        <childnode refid="7168" relation="include">
        </childnode>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7191" relation="include">
        </childnode>
        <childnode refid="7171" relation="include">
        </childnode>
        <childnode refid="7163" relation="include">
        </childnode>
        <childnode refid="7193" relation="include">
        </childnode>
        <childnode refid="7194" relation="include">
        </childnode>
      </node>
      <node id="7174">
        <label>iterator</label>
      </node>
      <node id="7186">
        <label>ATen/Scalar.h</label>
        <link refid="_scalar_8h_source"/>
        <childnode refid="7187" relation="include">
        </childnode>
        <childnode refid="7188" relation="include">
        </childnode>
        <childnode refid="7159" relation="include">
        </childnode>
        <childnode refid="7158" relation="include">
        </childnode>
        <childnode refid="7153" relation="include">
        </childnode>
        <childnode refid="7146" relation="include">
        </childnode>
        <childnode refid="7178" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7189" relation="include">
        </childnode>
        <childnode refid="7192" relation="include">
        </childnode>
      </node>
      <node id="7206">
        <label>TH/THStorageFunctions.hpp</label>
      </node>
      <node id="7180">
        <label>cstdint</label>
      </node>
      <node id="7214">
        <label>ATen/Deprecated.h</label>
        <link refid="_deprecated_8h_source"/>
      </node>
      <node id="7188">
        <label>stdint.h</label>
      </node>
      <node id="7225">
        <label>ATen/CUDAGuard.h</label>
        <link refid="_c_u_d_a_guard_8h_source"/>
      </node>
      <node id="7181">
        <label>cmath</label>
      </node>
      <node id="7156">
        <label>cassert</label>
      </node>
      <node id="7197">
        <label>ATen/Context.h</label>
        <link refid="_context_8h_source"/>
        <childnode refid="7146" relation="include">
        </childnode>
        <childnode refid="7147" relation="include">
        </childnode>
        <childnode refid="7196" relation="include">
        </childnode>
        <childnode refid="7195" relation="include">
        </childnode>
        <childnode refid="7192" relation="include">
        </childnode>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7198" relation="include">
        </childnode>
        <childnode refid="7204" relation="include">
        </childnode>
        <childnode refid="7149" relation="include">
        </childnode>
        <childnode refid="7201" relation="include">
        </childnode>
        <childnode refid="7180" relation="include">
        </childnode>
      </node>
      <node id="7183">
        <label>Half-inl.h</label>
        <link refid="_half-inl_8h_source"/>
        <childnode refid="7146" relation="include">
        </childnode>
        <childnode refid="7173" relation="include">
        </childnode>
        <childnode refid="7179" relation="include">
        </childnode>
      </node>
      <node id="7159">
        <label>stdexcept</label>
      </node>
      <node id="7220">
        <label>ATen/TensorOperators.h</label>
        <link refid="_tensor_operators_8h_source"/>
        <childnode refid="7186" relation="include">
        </childnode>
        <childnode refid="7207" relation="include">
        </childnode>
        <childnode refid="7195" relation="include">
        </childnode>
        <childnode refid="7158" relation="include">
        </childnode>
        <childnode refid="7159" relation="include">
        </childnode>
      </node>
      <node id="7194">
        <label>numeric</label>
      </node>
      <node id="7190">
        <label>ATen/TensorImpl.h</label>
        <link refid="_tensor_impl_8h_source"/>
        <childnode refid="7165" relation="include">
        </childnode>
        <childnode refid="7149" relation="include">
        </childnode>
        <childnode refid="7164" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7152" relation="include">
        </childnode>
      </node>
      <node id="7178">
        <label>ATen/Half.h</label>
        <link refid="_half_8h_source"/>
        <childnode refid="7146" relation="include">
        </childnode>
        <childnode refid="7179" relation="include">
        </childnode>
        <childnode refid="7158" relation="include">
        </childnode>
        <childnode refid="7180" relation="include">
        </childnode>
        <childnode refid="7159" relation="include">
        </childnode>
        <childnode refid="7153" relation="include">
        </childnode>
        <childnode refid="7181" relation="include">
        </childnode>
        <childnode refid="7182" relation="include">
        </childnode>
        <childnode refid="7183" relation="include">
        </childnode>
      </node>
      <node id="7164">
        <label>ATen/Retainable.h</label>
        <link refid="_retainable_8h_source"/>
        <childnode refid="7165" relation="include">
        </childnode>
      </node>
      <node id="7189">
        <label>ATen/TensorBase.h</label>
        <link refid="_tensor_base_8h_source"/>
        <childnode refid="7190" relation="include">
        </childnode>
        <childnode refid="7191" relation="include">
        </childnode>
      </node>
      <node id="7158">
        <label>string</label>
      </node>
      <node id="7223">
        <label>ATen/DimVector.h</label>
        <link refid="_dim_vector_8h_source"/>
        <childnode refid="7169" relation="include">
        </childnode>
        <childnode refid="7188" relation="include">
        </childnode>
      </node>
      <node id="7224">
        <label>ATen/OptionsGuard.h</label>
        <link refid="_options_guard_8h_source"/>
        <childnode refid="7166" relation="include">
        </childnode>
        <childnode refid="7210" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7217" relation="include">
        </childnode>
        <childnode refid="7152" relation="include">
        </childnode>
      </node>
      <node id="7233">
        <label>map</label>
      </node>
      <node id="7226">
        <label>ATen/ExpandUtils.h</label>
        <link refid="_expand_utils_8h_source"/>
        <childnode refid="7207" relation="include">
        </childnode>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7157" relation="include">
        </childnode>
        <childnode refid="7163" relation="include">
        </childnode>
        <childnode refid="7227" relation="include">
        </childnode>
      </node>
      <node id="7148">
        <label>ATen/Allocator.h</label>
        <link refid="_allocator_8h_source"/>
        <childnode refid="7149" relation="include">
        </childnode>
        <childnode refid="7150" relation="include">
        </childnode>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7164" relation="include">
        </childnode>
        <childnode refid="7166" relation="include">
        </childnode>
        <childnode refid="7185" relation="include">
        </childnode>
      </node>
      <node id="7204">
        <label>ATen/CUDAStream.h</label>
        <link refid="_c_u_d_a_stream_8h_source"/>
      </node>
      <node id="7216">
        <label>ATen/DeviceGuard.h</label>
        <link refid="_device_guard_8h_source"/>
        <childnode refid="7166" relation="include">
        </childnode>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7207" relation="include">
        </childnode>
        <childnode refid="7198" relation="include">
        </childnode>
        <childnode refid="7160" relation="include">
        </childnode>
      </node>
      <node id="7161">
        <label>exception</label>
      </node>
      <node id="7150">
        <label>stddef.h</label>
      </node>
      <node id="7191">
        <label>ATen/UndefinedTensor.h</label>
        <link refid="_undefined_tensor_8h_source"/>
        <childnode refid="7190" relation="include">
        </childnode>
      </node>
      <node id="7222">
        <label>ATen/Dispatch.h</label>
        <link refid="_dispatch_8h_source"/>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7178" relation="include">
        </childnode>
        <childnode refid="7195" relation="include">
        </childnode>
      </node>
      <node id="7170">
        <label>AlignOf.h</label>
        <link refid="_align_of_8h_source"/>
        <childnode refid="7160" relation="include">
        </childnode>
      </node>
      <node id="7205">
        <label>ATen/Storage.h</label>
        <link refid="_storage_8h_source"/>
        <childnode refid="7186" relation="include">
        </childnode>
        <childnode refid="7206" relation="include">
        </childnode>
      </node>
      <node id="7162">
        <label>ostream</label>
      </node>
      <node id="7147">
        <label>ATen/CPUGeneral.h</label>
        <link refid="_c_p_u_general_8h_source"/>
        <childnode refid="7146" relation="include">
        </childnode>
      </node>
      <node id="7195">
        <label>ATen/Type.h</label>
      </node>
      <node id="7168">
        <label>ATen/ArrayRef.h</label>
        <link refid="_array_ref_8h_source"/>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7169" relation="include">
        </childnode>
        <childnode refid="7176" relation="include">
        </childnode>
        <childnode refid="7174" relation="include">
        </childnode>
        <childnode refid="7177" relation="include">
        </childnode>
      </node>
      <node id="7211">
        <label>ATen/TensorGeometry.h</label>
        <link refid="_tensor_geometry_8h_source"/>
        <childnode refid="7195" relation="include">
        </childnode>
        <childnode refid="7212" relation="include">
        </childnode>
      </node>
      <node id="7208">
        <label>ATen/SparseTensorRef.h</label>
        <link refid="_sparse_tensor_ref_8h_source"/>
      </node>
      <node id="7146">
        <label>ATen/ATenGeneral.h</label>
        <link refid="_a_ten_general_8h_source"/>
      </node>
      <node id="7212">
        <label>ATen/WrapDimUtils.h</label>
        <link refid="_wrap_dim_utils_8h_source"/>
        <childnode refid="7190" relation="include">
        </childnode>
        <childnode refid="7163" relation="include">
        </childnode>
      </node>
      <node id="7209">
        <label>ATen/TensorAccessor.h</label>
        <link refid="_tensor_accessor_8h_source"/>
        <childnode refid="7160" relation="include">
        </childnode>
        <childnode refid="7188" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
      </node>
      <node id="7232">
        <label>ATen/native/DispatchStub.h</label>
        <link refid="_dispatch_stub_8h_source"/>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7154" relation="include">
        </childnode>
      </node>
      <node id="7193">
        <label>typeinfo</label>
      </node>
      <node id="7228">
        <label>ATen/WrapDimUtilsMulti.h</label>
        <link refid="_wrap_dim_utils_multi_8h_source"/>
        <childnode refid="7190" relation="include">
        </childnode>
        <childnode refid="7212" relation="include">
        </childnode>
        <childnode refid="7163" relation="include">
        </childnode>
        <childnode refid="7229" relation="include">
        </childnode>
      </node>
      <node id="7200">
        <label>cstdio</label>
      </node>
      <node id="7229">
        <label>bitset</label>
      </node>
      <node id="7227">
        <label>tuple</label>
      </node>
      <node id="7185">
        <label>ATen/detail/UniqueVoidPtr.h</label>
        <link refid="_unique_void_ptr_8h_source"/>
        <childnode refid="7149" relation="include">
        </childnode>
        <childnode refid="7146" relation="include">
        </childnode>
      </node>
      <node id="7177">
        <label>vector</label>
      </node>
      <node id="7221">
        <label>ATen/TensorMethods.h</label>
      </node>
      <node id="7218">
        <label>THNN/Reduction.h</label>
      </node>
      <node id="7175">
        <label>new</label>
      </node>
      <node id="7153">
        <label>utility</label>
      </node>
      <node id="7176">
        <label>array</label>
      </node>
      <node id="7163">
        <label>sstream</label>
      </node>
      <node id="7213">
        <label>ATen/Functions.h</label>
        <link refid="build_2aten_2src_2_a_ten_2_functions_8h_source"/>
        <childnode refid="7186" relation="include">
        </childnode>
        <childnode refid="7195" relation="include">
        </childnode>
        <childnode refid="7207" relation="include">
        </childnode>
        <childnode refid="7205" relation="include">
        </childnode>
        <childnode refid="7196" relation="include">
        </childnode>
        <childnode refid="7214" relation="include">
        </childnode>
        <childnode refid="7215" relation="include">
        </childnode>
        <childnode refid="7216" relation="include">
        </childnode>
        <childnode refid="7217" relation="include">
        </childnode>
        <childnode refid="7218" relation="include">
        </childnode>
      </node>
      <node id="7215">
        <label>ATen/NativeFunctions.h</label>
      </node>
      <node id="7182">
        <label>iosfwd</label>
      </node>
      <node id="7172">
        <label>cstdlib</label>
      </node>
      <node id="7231">
        <label>cpu/ReduceOpsKernel.h</label>
        <link refid="_reduce_ops_kernel_8h_source"/>
        <childnode refid="7145" relation="include">
        </childnode>
        <childnode refid="7232" relation="include">
        </childnode>
        <childnode refid="7152" relation="include">
        </childnode>
      </node>
      <node id="7184">
        <label>iostream</label>
      </node>
      <node id="7160">
        <label>cstddef</label>
      </node>
      <node id="7202">
        <label>unordered_map</label>
      </node>
      <node id="7201">
        <label>mutex</label>
      </node>
      <node id="7230">
        <label>ReduceOpsUtils.h</label>
        <link refid="_reduce_ops_utils_8h_source"/>
      </node>
      <node id="7145">
        <label>ATen/ATen.h</label>
        <link refid="_a_ten_8h_source"/>
        <childnode refid="7146" relation="include">
        </childnode>
        <childnode refid="7147" relation="include">
        </childnode>
        <childnode refid="7148" relation="include">
        </childnode>
        <childnode refid="7186" relation="include">
        </childnode>
        <childnode refid="7195" relation="include">
        </childnode>
        <childnode refid="7196" relation="include">
        </childnode>
        <childnode refid="7197" relation="include">
        </childnode>
        <childnode refid="7205" relation="include">
        </childnode>
        <childnode refid="7207" relation="include">
        </childnode>
        <childnode refid="7166" relation="include">
        </childnode>
        <childnode refid="7211" relation="include">
        </childnode>
        <childnode refid="7213" relation="include">
        </childnode>
        <childnode refid="7219" relation="include">
        </childnode>
        <childnode refid="7220" relation="include">
        </childnode>
        <childnode refid="7221" relation="include">
        </childnode>
        <childnode refid="7222" relation="include">
        </childnode>
        <childnode refid="7223" relation="include">
        </childnode>
        <childnode refid="7216" relation="include">
        </childnode>
        <childnode refid="7217" relation="include">
        </childnode>
        <childnode refid="7210" relation="include">
        </childnode>
        <childnode refid="7224" relation="include">
        </childnode>
        <childnode refid="7225" relation="include">
        </childnode>
      </node>
      <node id="7199">
        <label>ATen/Registry.h</label>
        <link refid="_registry_8h_source"/>
        <childnode refid="7171" relation="include">
        </childnode>
        <childnode refid="7200" relation="include">
        </childnode>
        <childnode refid="7172" relation="include">
        </childnode>
        <childnode refid="7157" relation="include">
        </childnode>
        <childnode refid="7149" relation="include">
        </childnode>
        <childnode refid="7201" relation="include">
        </childnode>
        <childnode refid="7202" relation="include">
        </childnode>
        <childnode refid="7158" relation="include">
        </childnode>
        <childnode refid="7177" relation="include">
        </childnode>
        <childnode refid="7203" relation="include">
        </childnode>
        <childnode refid="7146" relation="include">
        </childnode>
      </node>
      <node id="7151">
        <label>ATen/Error.h</label>
        <link refid="_error_8h_source"/>
        <childnode refid="7146" relation="include">
        </childnode>
        <childnode refid="7152" relation="include">
        </childnode>
        <childnode refid="7160" relation="include">
        </childnode>
        <childnode refid="7161" relation="include">
        </childnode>
        <childnode refid="7162" relation="include">
        </childnode>
        <childnode refid="7163" relation="include">
        </childnode>
        <childnode refid="7158" relation="include">
        </childnode>
      </node>
      <node id="7217">
        <label>ATen/TensorOptions.h</label>
        <link refid="_tensor_options_8h_source"/>
        <childnode refid="7197" relation="include">
        </childnode>
        <childnode refid="7166" relation="include">
        </childnode>
        <childnode refid="7216" relation="include">
        </childnode>
        <childnode refid="7210" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7207" relation="include">
        </childnode>
        <childnode refid="7195" relation="include">
        </childnode>
        <childnode refid="7160" relation="include">
        </childnode>
        <childnode refid="7182" relation="include">
        </childnode>
        <childnode refid="7153" relation="include">
        </childnode>
      </node>
      <node id="7207">
        <label>ATen/Tensor.h</label>
        <link refid="build_2aten_2src_2_a_ten_2tensor_8h_source"/>
        <childnode refid="7196" relation="include">
        </childnode>
        <childnode refid="7186" relation="include">
        </childnode>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7208" relation="include">
        </childnode>
        <childnode refid="7205" relation="include">
        </childnode>
        <childnode refid="7209" relation="include">
        </childnode>
        <childnode refid="7189" relation="include">
        </childnode>
        <childnode refid="7190" relation="include">
        </childnode>
        <childnode refid="7192" relation="include">
        </childnode>
        <childnode refid="7166" relation="include">
        </childnode>
        <childnode refid="7210" relation="include">
        </childnode>
        <childnode refid="7152" relation="include">
        </childnode>
      </node>
      <node id="7154">
        <label>type_traits</label>
      </node>
      <node id="7210">
        <label>ATen/Layout.h</label>
        <link refid="_layout_8h_source"/>
        <childnode refid="7167" relation="include">
        </childnode>
        <childnode refid="7151" relation="include">
        </childnode>
        <childnode refid="7184" relation="include">
        </childnode>
      </node>
      <node id="7144">
        <label>/Users/robkunkle/fork/goodlux/pytorch/aten/src/ATen/native/ReduceOps.cpp</label>
        <link refid="_reduce_ops_8cpp"/>
        <childnode refid="7145" relation="include">
        </childnode>
        <childnode refid="7222" relation="include">
        </childnode>
        <childnode refid="7226" relation="include">
        </childnode>
        <childnode refid="7215" relation="include">
        </childnode>
        <childnode refid="7212" relation="include">
        </childnode>
        <childnode refid="7228" relation="include">
        </childnode>
        <childnode refid="7230" relation="include">
        </childnode>
        <childnode refid="7231" relation="include">
        </childnode>
        <childnode refid="7171" relation="include">
        </childnode>
        <childnode refid="7157" relation="include">
        </childnode>
        <childnode refid="7179" relation="include">
        </childnode>
        <childnode refid="7194" relation="include">
        </childnode>
        <childnode refid="7177" relation="include">
        </childnode>
        <childnode refid="7233" relation="include">
        </childnode>
      </node>
      <node id="7169">
        <label>ATen/SmallVector.h</label>
        <link refid="_small_vector_8h_source"/>
        <childnode refid="7170" relation="include">
        </childnode>
        <childnode refid="7171" relation="include">
        </childnode>
        <childnode refid="7156" relation="include">
        </childnode>
        <childnode refid="7160" relation="include">
        </childnode>
        <childnode refid="7172" relation="include">
        </childnode>
        <childnode refid="7173" relation="include">
        </childnode>
        <childnode refid="7155" relation="include">
        </childnode>
        <childnode refid="7174" relation="include">
        </childnode>
        <childnode refid="7149" relation="include">
        </childnode>
        <childnode refid="7175" relation="include">
        </childnode>
        <childnode refid="7154" relation="include">
        </childnode>
        <childnode refid="7153" relation="include">
        </childnode>
        <childnode refid="7146" relation="include">
        </childnode>
      </node>
      <node id="7171">
        <label>algorithm</label>
      </node>
      <node id="7196">
        <label>ATen/Generator.h</label>
        <link refid="_generator_8h_source"/>
        <childnode refid="7188" relation="include">
        </childnode>
      </node>
      <node id="7173">
        <label>cstring</label>
      </node>
      <node id="7149">
        <label>memory</label>
      </node>
      <node id="7155">
        <label>initializer_list</label>
      </node>
      <node id="7219">
        <label>ATen/Formatting.h</label>
        <link refid="_formatting_8h_source"/>
        <childnode refid="7184" relation="include">
        </childnode>
        <childnode refid="7195" relation="include">
        </childnode>
        <childnode refid="7186" relation="include">
        </childnode>
      </node>
    </incdepgraph>
    <innernamespace refid="namespaceat">at</innernamespace>
    <innernamespace refid="namespaceat_1_1native">at::native</innernamespace>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
    </detaileddescription>
    <programlisting>
<codeline lineno="1"><highlight class="preprocessor">#include<sp/>&quot;ATen/ATen.h&quot;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="2"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&quot;ATen/Dispatch.h&quot;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="3"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&quot;ATen/ExpandUtils.h&quot;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="4"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&quot;ATen/NativeFunctions.h&quot;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="5"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&quot;ATen/WrapDimUtils.h&quot;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="6"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&quot;ATen/WrapDimUtilsMulti.h&quot;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="7"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&quot;ReduceOpsUtils.h&quot;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="8"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&quot;cpu/ReduceOpsKernel.h&quot;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="9"><highlight class="normal"></highlight></codeline>
<codeline lineno="10"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;algorithm&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="11"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;functional&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="12"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;limits&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="13"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;numeric&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="14"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;vector&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="15"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;map&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="16"><highlight class="normal"></highlight></codeline>
<codeline lineno="17"><highlight class="normal"></highlight><highlight class="keyword">namespace<sp/></highlight><highlight class="normal"><ref refid="namespaceat" kindref="compound">at</ref><sp/>{</highlight></codeline>
<codeline lineno="18"><highlight class="normal"></highlight><highlight class="keyword">namespace<sp/></highlight><highlight class="normal">native<sp/>{</highlight></codeline>
<codeline lineno="19"><highlight class="normal"></highlight></codeline>
<codeline lineno="20"><highlight class="normal">DEFINE_DISPATCH(sum_kernel);</highlight></codeline>
<codeline lineno="21"><highlight class="normal">DEFINE_DISPATCH(prod_kernel);</highlight></codeline>
<codeline lineno="22"><highlight class="normal"></highlight></codeline>
<codeline lineno="23"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>integer_upcast(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="24"><highlight class="normal"><sp/><sp/>ScalarType<sp/>scalarType<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType();</highlight></codeline>
<codeline lineno="25"><highlight class="normal"><sp/><sp/>ScalarType<sp/>upcast_scalarType<sp/>=<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value_or(at::isIntegralType(scalarType)<sp/>?<sp/>ScalarType::Long<sp/>:<sp/>scalarType);</highlight></codeline>
<codeline lineno="26"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.toType(upcast_scalarType);</highlight></codeline>
<codeline lineno="27"><highlight class="normal">}</highlight></codeline>
<codeline lineno="28"><highlight class="normal"></highlight></codeline>
<codeline lineno="29"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>cumsum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="30"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_cumsum(integer_upcast(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>),<sp/>dim);</highlight></codeline>
<codeline lineno="31"><highlight class="normal">}</highlight></codeline>
<codeline lineno="32"><highlight class="normal"></highlight></codeline>
<codeline lineno="33"><highlight class="normal">Tensor<sp/>cumsum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="34"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::cumsum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>optional&lt;ScalarType&gt;(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="35"><highlight class="normal">}</highlight></codeline>
<codeline lineno="36"><highlight class="normal"></highlight></codeline>
<codeline lineno="37"><highlight class="normal">Tensor<sp/>cumsum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim)<sp/>{</highlight></codeline>
<codeline lineno="38"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::cumsum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="39"><highlight class="normal">}</highlight></codeline>
<codeline lineno="40"><highlight class="normal"></highlight></codeline>
<codeline lineno="41"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/>cumsum_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="42"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>result<sp/>type<sp/>is<sp/>favored<sp/>over<sp/>dtype;<sp/>check<sp/>that<sp/>they<sp/>match<sp/>if<sp/>provided<sp/>(NumPy<sp/>doesn&apos;t<sp/>check)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="43"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight></codeline>
<codeline lineno="44"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>!<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.has_value()<sp/>||<sp/>(result.type().scalarType()<sp/>==<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value()),</highlight></codeline>
<codeline lineno="45"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;provided<sp/>dtype<sp/>must<sp/>match<sp/>dtype<sp/>of<sp/>result<sp/>in<sp/>cumsum.<sp/>Got<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="46"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(result.type().scalarType()),</highlight></codeline>
<codeline lineno="47"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;<sp/>and<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="48"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value()),</highlight></codeline>
<codeline lineno="49"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;.&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="50"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_cumsum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.toType(result.type().scalarType()),<sp/>dim);</highlight></codeline>
<codeline lineno="51"><highlight class="normal">}</highlight></codeline>
<codeline lineno="52"><highlight class="normal"></highlight></codeline>
<codeline lineno="53"><highlight class="normal">Tensor&amp;<sp/>cumsum_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="54"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::cumsum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>optional&lt;ScalarType&gt;(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="55"><highlight class="normal">}</highlight></codeline>
<codeline lineno="56"><highlight class="normal"></highlight></codeline>
<codeline lineno="57"><highlight class="normal">Tensor&amp;<sp/>cumsum_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim)<sp/>{</highlight></codeline>
<codeline lineno="58"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::cumsum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="59"><highlight class="normal">}</highlight></codeline>
<codeline lineno="60"><highlight class="normal"></highlight></codeline>
<codeline lineno="61"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>cumprod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="62"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_cumprod(integer_upcast(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>),<sp/>dim);</highlight></codeline>
<codeline lineno="63"><highlight class="normal">}</highlight></codeline>
<codeline lineno="64"><highlight class="normal"></highlight></codeline>
<codeline lineno="65"><highlight class="normal">Tensor<sp/>cumprod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="66"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::cumprod(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>optional&lt;ScalarType&gt;(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="67"><highlight class="normal">}</highlight></codeline>
<codeline lineno="68"><highlight class="normal"></highlight></codeline>
<codeline lineno="69"><highlight class="normal">Tensor<sp/>cumprod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim)<sp/>{</highlight></codeline>
<codeline lineno="70"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::cumprod(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="71"><highlight class="normal">}</highlight></codeline>
<codeline lineno="72"><highlight class="normal"></highlight></codeline>
<codeline lineno="73"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/>cumprod_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="74"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>result<sp/>type<sp/>is<sp/>favored<sp/>over<sp/>dtype;<sp/>check<sp/>that<sp/>they<sp/>match<sp/>if<sp/>provided<sp/>(NumPy<sp/>doesn&apos;t<sp/>check)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="75"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight></codeline>
<codeline lineno="76"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>!<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.has_value()<sp/>||<sp/>(result.type().scalarType()<sp/>==<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value()),</highlight></codeline>
<codeline lineno="77"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;provided<sp/>dtype<sp/>must<sp/>match<sp/>dtype<sp/>of<sp/>result<sp/>in<sp/>cumprod.<sp/>Got<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="78"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(result.type().scalarType()),</highlight></codeline>
<codeline lineno="79"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;<sp/>and<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="80"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value()),</highlight></codeline>
<codeline lineno="81"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;.&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="82"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_cumprod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.toType(result.type().scalarType()),<sp/>dim);</highlight></codeline>
<codeline lineno="83"><highlight class="normal">}</highlight></codeline>
<codeline lineno="84"><highlight class="normal"></highlight></codeline>
<codeline lineno="85"><highlight class="normal">Tensor&amp;<sp/>cumprod_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="86"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::cumprod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>optional&lt;ScalarType&gt;(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="87"><highlight class="normal">}</highlight></codeline>
<codeline lineno="88"><highlight class="normal"></highlight></codeline>
<codeline lineno="89"><highlight class="normal">Tensor&amp;<sp/>cumprod_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim)<sp/>{</highlight></codeline>
<codeline lineno="90"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::cumprod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="91"><highlight class="normal">}</highlight></codeline>
<codeline lineno="92"><highlight class="normal"></highlight></codeline>
<codeline lineno="93"><highlight class="normal"></highlight><highlight class="comment">//<sp/>ALL<sp/>REDUCE<sp/>#################################################################</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="94"><highlight class="normal"></highlight></codeline>
<codeline lineno="95"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>mean(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="96"><highlight class="normal"><sp/><sp/>ScalarType<sp/>scalarType<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType();</highlight></codeline>
<codeline lineno="97"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight></codeline>
<codeline lineno="98"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::isFloatingType(scalarType),</highlight></codeline>
<codeline lineno="99"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;Can<sp/>only<sp/>calculate<sp/>the<sp/>mean<sp/>of<sp/>floating<sp/>types.<sp/>Got<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="100"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(scalarType),</highlight></codeline>
<codeline lineno="101"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;<sp/>instead.&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="102"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.numel()<sp/>&gt;<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="103"><highlight class="normal"><sp/><sp/><sp/><sp/>Tensor<sp/>result<sp/>=<sp/>at::native::sum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="104"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result.div_(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.numel());</highlight></codeline>
<codeline lineno="105"><highlight class="normal"><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="106"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarTensor(std::numeric_limits&lt;double&gt;::quiet_NaN());</highlight></codeline>
<codeline lineno="107"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="108"><highlight class="normal">}</highlight></codeline>
<codeline lineno="109"><highlight class="normal"></highlight></codeline>
<codeline lineno="110"><highlight class="normal">Tensor<sp/>mean(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="111"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::mean(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>optional&lt;ScalarType&gt;(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="112"><highlight class="normal">}</highlight></codeline>
<codeline lineno="113"><highlight class="normal"></highlight></codeline>
<codeline lineno="114"><highlight class="normal">Tensor<sp/>mean(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">)<sp/>{</highlight></codeline>
<codeline lineno="115"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::mean(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>nullopt);</highlight></codeline>
<codeline lineno="116"><highlight class="normal">}</highlight></codeline>
<codeline lineno="117"><highlight class="normal"></highlight></codeline>
<codeline lineno="118"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="119"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_sum(integer_upcast(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="120"><highlight class="normal">}</highlight></codeline>
<codeline lineno="121"><highlight class="normal"></highlight></codeline>
<codeline lineno="122"><highlight class="normal">Tensor<sp/>sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="123"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::sum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>optional&lt;ScalarType&gt;(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="124"><highlight class="normal">}</highlight></codeline>
<codeline lineno="125"><highlight class="normal"></highlight></codeline>
<codeline lineno="126"><highlight class="normal">Tensor<sp/>sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">)<sp/>{</highlight></codeline>
<codeline lineno="127"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::sum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>nullopt);</highlight></codeline>
<codeline lineno="128"><highlight class="normal">}</highlight></codeline>
<codeline lineno="129"><highlight class="normal"></highlight></codeline>
<codeline lineno="130"><highlight class="normal">Tensor<sp/>_sum_cpu(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">)<sp/>{</highlight></codeline>
<codeline lineno="131"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.is_contiguous())<sp/>{</highlight></codeline>
<codeline lineno="132"><highlight class="normal"><sp/><sp/><sp/><sp/>Tensor<sp/>result<sp/>=<sp/>at::empty({},<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type());</highlight></codeline>
<codeline lineno="133"><highlight class="normal"><sp/><sp/><sp/><sp/>sum_kernel(kCPU,<sp/>result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>at::nullopt);</highlight></codeline>
<codeline lineno="134"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="135"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="136"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">._sumall();</highlight></codeline>
<codeline lineno="137"><highlight class="normal">}</highlight></codeline>
<codeline lineno="138"><highlight class="normal"></highlight></codeline>
<codeline lineno="139"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>prod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="140"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_prod(integer_upcast(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="141"><highlight class="normal">}</highlight></codeline>
<codeline lineno="142"><highlight class="normal"></highlight></codeline>
<codeline lineno="143"><highlight class="normal">Tensor<sp/>prod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="144"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::prod(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>optional&lt;ScalarType&gt;(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="145"><highlight class="normal">}</highlight></codeline>
<codeline lineno="146"><highlight class="normal"></highlight></codeline>
<codeline lineno="147"><highlight class="normal">Tensor<sp/>prod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">)<sp/>{</highlight></codeline>
<codeline lineno="148"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::prod(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>nullopt);</highlight></codeline>
<codeline lineno="149"><highlight class="normal">}</highlight></codeline>
<codeline lineno="150"><highlight class="normal"></highlight></codeline>
<codeline lineno="151"><highlight class="normal">Tensor<sp/>_prod_cpu(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">)<sp/>{</highlight></codeline>
<codeline lineno="152"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.is_contiguous())<sp/>{</highlight></codeline>
<codeline lineno="153"><highlight class="normal"><sp/><sp/><sp/><sp/>Tensor<sp/>result<sp/>=<sp/>at::empty({},<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type());</highlight></codeline>
<codeline lineno="154"><highlight class="normal"><sp/><sp/><sp/><sp/>prod_kernel(kCPU,<sp/>result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>at::nullopt);</highlight></codeline>
<codeline lineno="155"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="156"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="157"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">._prodall();</highlight></codeline>
<codeline lineno="158"><highlight class="normal">}</highlight></codeline>
<codeline lineno="159"><highlight class="normal"></highlight></codeline>
<codeline lineno="160"><highlight class="normal"></highlight><highlight class="comment">//<sp/>\ALL<sp/>REDUCE<sp/>################################################################</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="161"><highlight class="normal"></highlight></codeline>
<codeline lineno="162"><highlight class="normal"></highlight><highlight class="comment">//<sp/>DIM<sp/>REDUCE<sp/>#################################################################</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="163"><highlight class="normal"></highlight></codeline>
<codeline lineno="164"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;mean_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,</highlight></codeline>
<codeline lineno="165"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="166"><highlight class="normal"><sp/><sp/>ScalarType<sp/>scalarType<sp/>=<sp/>result.type().scalarType();</highlight></codeline>
<codeline lineno="167"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight></codeline>
<codeline lineno="168"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::isFloatingType(scalarType),</highlight></codeline>
<codeline lineno="169"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;Can<sp/>only<sp/>calculate<sp/>the<sp/>mean<sp/>of<sp/>floating<sp/>types.<sp/>Got<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="170"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(scalarType),</highlight></codeline>
<codeline lineno="171"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;<sp/>instead.&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="172"><highlight class="normal"><sp/><sp/>at::native::sum_out(</highlight></codeline>
<codeline lineno="173"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.toType(result.type().scalarType()),<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="174"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(result.numel()<sp/>&gt;<sp/>0<sp/>&amp;&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.ndimension()<sp/>&gt;<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="175"><highlight class="normal"><sp/><sp/><sp/><sp/>int64_t<sp/>numel<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.size(dim);</highlight></codeline>
<codeline lineno="176"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(numel<sp/>&gt;<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="177"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>result.div_(numel);</highlight></codeline>
<codeline lineno="178"><highlight class="normal"><sp/><sp/><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="179"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>NumPy<sp/>equivalent</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="180"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>result.fill_(std::numeric_limits&lt;double&gt;::quiet_NaN());</highlight></codeline>
<codeline lineno="181"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="182"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="183"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="184"><highlight class="normal">}</highlight></codeline>
<codeline lineno="185"><highlight class="normal"></highlight></codeline>
<codeline lineno="186"><highlight class="normal">Tensor&amp;<sp/>mean_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="187"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::mean_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/><ref refid="classat_1_1optional" kindref="compound">at::optional&lt;ScalarType&gt;</ref>(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="188"><highlight class="normal">}</highlight></codeline>
<codeline lineno="189"><highlight class="normal">Tensor&amp;<sp/>mean_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="190"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::mean_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="191"><highlight class="normal">}</highlight></codeline>
<codeline lineno="192"><highlight class="normal"></highlight></codeline>
<codeline lineno="193"><highlight class="normal">Tensor&amp;<sp/>mean_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="194"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::mean_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/></highlight><highlight class="keyword">false</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>);</highlight></codeline>
<codeline lineno="195"><highlight class="normal">}</highlight></codeline>
<codeline lineno="196"><highlight class="normal"></highlight></codeline>
<codeline lineno="197"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;sum_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dim,</highlight></codeline>
<codeline lineno="198"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="199"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>result<sp/>type<sp/>is<sp/>favored<sp/>over<sp/>dtype;<sp/>check<sp/>that<sp/>they<sp/>match<sp/>if<sp/>provided<sp/>(NumPy<sp/>doesn&apos;t<sp/>check)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="200"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight></codeline>
<codeline lineno="201"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>!<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.has_value()<sp/>||<sp/>(result.type().scalarType()<sp/>==<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value()),</highlight></codeline>
<codeline lineno="202"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;provided<sp/>dtype<sp/>must<sp/>match<sp/>dtype<sp/>of<sp/>result<sp/>in<sp/>sum.<sp/>Got<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="203"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(result.type().scalarType()),</highlight></codeline>
<codeline lineno="204"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;<sp/>and<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="205"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value()),</highlight></codeline>
<codeline lineno="206"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;.&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="207"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_sum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.toType(result.type().scalarType()),<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="208"><highlight class="normal">}</highlight></codeline>
<codeline lineno="209"><highlight class="normal"></highlight></codeline>
<codeline lineno="210"><highlight class="normal">Tensor&amp;<sp/>sum_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="211"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::sum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/><ref refid="classat_1_1optional" kindref="compound">at::optional&lt;ScalarType&gt;</ref>(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="212"><highlight class="normal">}</highlight></codeline>
<codeline lineno="213"><highlight class="normal">Tensor&amp;<sp/>sum_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="214"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::sum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="215"><highlight class="normal">}</highlight></codeline>
<codeline lineno="216"><highlight class="normal"></highlight></codeline>
<codeline lineno="217"><highlight class="normal">Tensor&amp;<sp/>sum_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="218"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::sum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/></highlight><highlight class="keyword">false</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>);</highlight></codeline>
<codeline lineno="219"><highlight class="normal">}</highlight></codeline>
<codeline lineno="220"><highlight class="normal"></highlight></codeline>
<codeline lineno="221"><highlight class="normal">Tensor<sp/>&amp;_sum_out_cpu(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim_,</highlight></codeline>
<codeline lineno="222"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="223"><highlight class="normal"><sp/><sp/>int64_t<sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim_,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="224"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(_dimreduce_return_trivial(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>0,<sp/>dim,<sp/>keepdim))</highlight></codeline>
<codeline lineno="225"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="226"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.is_contiguous()<sp/>&amp;&amp;<sp/>result.is_contiguous())<sp/>{</highlight></codeline>
<codeline lineno="227"><highlight class="normal"><sp/><sp/><sp/><sp/>_dimreduce_setup(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim);</highlight></codeline>
<codeline lineno="228"><highlight class="normal"><sp/><sp/><sp/><sp/>sum_kernel(kCPU,<sp/>result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim);</highlight></codeline>
<codeline lineno="229"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!keepdim)<sp/>result.squeeze_(dim);</highlight></codeline>
<codeline lineno="230"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="231"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="232"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_th_sum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="233"><highlight class="normal">}</highlight></codeline>
<codeline lineno="234"><highlight class="normal"></highlight></codeline>
<codeline lineno="235"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;prod_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,</highlight></codeline>
<codeline lineno="236"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="237"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>result<sp/>type<sp/>is<sp/>favored<sp/>over<sp/>dtype;<sp/>check<sp/>that<sp/>they<sp/>match<sp/>if<sp/>provided<sp/>(NumPy<sp/>doesn&apos;t<sp/>check)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="238"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight></codeline>
<codeline lineno="239"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>!<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.has_value()<sp/>||<sp/>(result.type().scalarType()<sp/>==<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value()),</highlight></codeline>
<codeline lineno="240"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;provided<sp/>dtype<sp/>must<sp/>match<sp/>dtype<sp/>of<sp/>result<sp/>in<sp/>prod.<sp/>Got<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="241"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(result.type().scalarType()),</highlight></codeline>
<codeline lineno="242"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;<sp/>and<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="243"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>.value()),</highlight></codeline>
<codeline lineno="244"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;.&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="245"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_prod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.toType(result.type().scalarType()),<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="246"><highlight class="normal">}</highlight></codeline>
<codeline lineno="247"><highlight class="normal"></highlight></codeline>
<codeline lineno="248"><highlight class="normal">Tensor&amp;<sp/>prod_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="249"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::prod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/><ref refid="classat_1_1optional" kindref="compound">at::optional&lt;ScalarType&gt;</ref>(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="250"><highlight class="normal">}</highlight></codeline>
<codeline lineno="251"><highlight class="normal">Tensor&amp;<sp/>prod_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="252"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::prod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="253"><highlight class="normal">}</highlight></codeline>
<codeline lineno="254"><highlight class="normal"></highlight></codeline>
<codeline lineno="255"><highlight class="normal">Tensor&amp;<sp/>prod_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="256"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::prod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/></highlight><highlight class="keyword">false</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>);</highlight></codeline>
<codeline lineno="257"><highlight class="normal">}</highlight></codeline>
<codeline lineno="258"><highlight class="normal"></highlight></codeline>
<codeline lineno="259"><highlight class="normal">Tensor<sp/>&amp;_prod_out_cpu(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim_,</highlight></codeline>
<codeline lineno="260"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="261"><highlight class="normal"><sp/><sp/>int64_t<sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim_,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="262"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(_dimreduce_return_trivial(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>1,<sp/>dim,<sp/>keepdim))</highlight></codeline>
<codeline lineno="263"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="264"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.is_contiguous()<sp/>&amp;&amp;<sp/>result.is_contiguous())<sp/>{</highlight></codeline>
<codeline lineno="265"><highlight class="normal"><sp/><sp/><sp/><sp/>_dimreduce_setup(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim);</highlight></codeline>
<codeline lineno="266"><highlight class="normal"><sp/><sp/><sp/><sp/>prod_kernel(kCPU,<sp/>result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim);</highlight></codeline>
<codeline lineno="267"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!keepdim)<sp/>result.squeeze_(dim);</highlight></codeline>
<codeline lineno="268"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="269"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="270"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_th_prod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="271"><highlight class="normal">}</highlight></codeline>
<codeline lineno="272"><highlight class="normal"></highlight></codeline>
<codeline lineno="273"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>mean(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="274"><highlight class="normal"><sp/><sp/>ScalarType<sp/>scalarType<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType();</highlight></codeline>
<codeline lineno="275"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight></codeline>
<codeline lineno="276"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::isFloatingType(scalarType),</highlight></codeline>
<codeline lineno="277"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;Can<sp/>only<sp/>calculate<sp/>the<sp/>mean<sp/>of<sp/>floating<sp/>types.<sp/>Got<sp/>&quot;</highlight><highlight class="normal">,</highlight></codeline>
<codeline lineno="278"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>at::toString(scalarType),</highlight></codeline>
<codeline lineno="279"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;<sp/>instead.&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="280"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/>at::native::sum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="281"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(result.numel()<sp/>&gt;<sp/>0<sp/>&amp;&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.ndimension()<sp/>&gt;<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="282"><highlight class="normal"><sp/><sp/><sp/><sp/>int64_t<sp/>numel<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.size(dim);</highlight></codeline>
<codeline lineno="283"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(numel<sp/>&gt;<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="284"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>result.div_(numel);</highlight></codeline>
<codeline lineno="285"><highlight class="normal"><sp/><sp/><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="286"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>NumPy<sp/>equivalent</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="287"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>result.fill_(std::numeric_limits&lt;double&gt;::quiet_NaN());</highlight></codeline>
<codeline lineno="288"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="289"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="290"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="291"><highlight class="normal">}</highlight></codeline>
<codeline lineno="292"><highlight class="normal"></highlight></codeline>
<codeline lineno="293"><highlight class="normal">Tensor<sp/>mean(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="294"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::mean(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/><ref refid="classat_1_1optional" kindref="compound">at::optional&lt;ScalarType&gt;</ref>(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="295"><highlight class="normal">}</highlight></codeline>
<codeline lineno="296"><highlight class="normal"></highlight></codeline>
<codeline lineno="297"><highlight class="normal">Tensor<sp/>mean(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="298"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::mean(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="299"><highlight class="normal">}</highlight></codeline>
<codeline lineno="300"><highlight class="normal"></highlight></codeline>
<codeline lineno="301"><highlight class="normal">Tensor<sp/>mean(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="302"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::mean(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/></highlight><highlight class="keyword">false</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>);</highlight></codeline>
<codeline lineno="303"><highlight class="normal">}</highlight></codeline>
<codeline lineno="304"><highlight class="normal"></highlight></codeline>
<codeline lineno="305"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dim_,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="306"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_sum(integer_upcast(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>),<sp/>dim_,<sp/>keepdim);</highlight></codeline>
<codeline lineno="307"><highlight class="normal">}</highlight></codeline>
<codeline lineno="308"><highlight class="normal"></highlight></codeline>
<codeline lineno="309"><highlight class="normal">Tensor<sp/>sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="310"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::sum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/><ref refid="classat_1_1optional" kindref="compound">at::optional&lt;ScalarType&gt;</ref>(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="311"><highlight class="normal">}</highlight></codeline>
<codeline lineno="312"><highlight class="normal"></highlight></codeline>
<codeline lineno="313"><highlight class="normal">Tensor<sp/>sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="314"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::sum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="315"><highlight class="normal">}</highlight></codeline>
<codeline lineno="316"><highlight class="normal"></highlight></codeline>
<codeline lineno="317"><highlight class="normal">Tensor<sp/>sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="318"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::sum(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/></highlight><highlight class="keyword">false</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>);</highlight></codeline>
<codeline lineno="319"><highlight class="normal">}</highlight></codeline>
<codeline lineno="320"><highlight class="normal"></highlight></codeline>
<codeline lineno="321"><highlight class="normal">Tensor<sp/>_sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim_,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="322"><highlight class="normal"><sp/><sp/>int64_t<sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim_,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="323"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().tensor();</highlight></codeline>
<codeline lineno="324"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_sum_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="325"><highlight class="normal">}</highlight></codeline>
<codeline lineno="326"><highlight class="normal"></highlight></codeline>
<codeline lineno="327"><highlight class="normal"></highlight><highlight class="keyword">static</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>prod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim_,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>optional&lt;ScalarType&gt;<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="328"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_prod(integer_upcast(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>),<sp/>dim_,<sp/>keepdim);</highlight></codeline>
<codeline lineno="329"><highlight class="normal">}</highlight></codeline>
<codeline lineno="330"><highlight class="normal"></highlight></codeline>
<codeline lineno="331"><highlight class="normal">Tensor<sp/>prod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="332"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::prod(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/><ref refid="classat_1_1optional" kindref="compound">at::optional&lt;ScalarType&gt;</ref>(<ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>));</highlight></codeline>
<codeline lineno="333"><highlight class="normal">}</highlight></codeline>
<codeline lineno="334"><highlight class="normal"></highlight></codeline>
<codeline lineno="335"><highlight class="normal">Tensor<sp/>prod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="336"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::prod(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim,<sp/>nullopt);</highlight></codeline>
<codeline lineno="337"><highlight class="normal">}</highlight></codeline>
<codeline lineno="338"><highlight class="normal"></highlight></codeline>
<codeline lineno="339"><highlight class="normal">Tensor<sp/>prod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/>ScalarType<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>)<sp/>{</highlight></codeline>
<codeline lineno="340"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::prod(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/></highlight><highlight class="keyword">false</highlight><highlight class="normal">,<sp/><ref refid="namespaceat_1a25549deef81727b261b7b298eb848197" kindref="member">dtype</ref>);</highlight></codeline>
<codeline lineno="341"><highlight class="normal">}</highlight></codeline>
<codeline lineno="342"><highlight class="normal"></highlight></codeline>
<codeline lineno="343"><highlight class="normal">Tensor<sp/>_prod(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim_,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="344"><highlight class="normal"><sp/><sp/>int64_t<sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim_,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="345"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().tensor();</highlight></codeline>
<codeline lineno="346"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_prod_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="347"><highlight class="normal">}</highlight></codeline>
<codeline lineno="348"><highlight class="normal"></highlight></codeline>
<codeline lineno="349"><highlight class="normal">Tensor&amp;<sp/>logsumexp_out(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim_,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="350"><highlight class="normal"><sp/><sp/>int64_t<sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim_,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="351"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>can&apos;t<sp/>take<sp/>max<sp/>of<sp/>empty<sp/>tensor.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="352"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.numel()<sp/>!=<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="353"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>maxes<sp/>=<sp/>at::max_values(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/></highlight><highlight class="keyword">true</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="354"><highlight class="normal"><sp/><sp/><sp/><sp/>result<sp/>=<sp/>at::where((maxes<sp/>==<sp/>INFINITY).__or__(maxes<sp/>==<sp/>-INFINITY),</highlight></codeline>
<codeline lineno="355"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>maxes,</highlight></codeline>
<codeline lineno="356"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>maxes<sp/>+<sp/>at::log(at::sum(at::exp(</highlight><highlight class="keyword">self</highlight><highlight class="normal"><sp/>-<sp/>maxes),<sp/>dim,<sp/></highlight><highlight class="keyword">true</highlight><highlight class="normal">)));</highlight></codeline>
<codeline lineno="357"><highlight class="normal"><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="358"><highlight class="normal"><sp/><sp/><sp/><sp/>result<sp/>=<sp/>at::log(at::sum(at::exp(</highlight><highlight class="keyword">self</highlight><highlight class="normal">),<sp/>dim,<sp/></highlight><highlight class="keyword">true</highlight><highlight class="normal">));</highlight></codeline>
<codeline lineno="359"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="360"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!<sp/>keepdim)</highlight></codeline>
<codeline lineno="361"><highlight class="normal"><sp/><sp/><sp/><sp/>result.squeeze_(dim);</highlight></codeline>
<codeline lineno="362"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="363"><highlight class="normal">}</highlight></codeline>
<codeline lineno="364"><highlight class="normal"></highlight></codeline>
<codeline lineno="365"><highlight class="normal">Tensor<sp/>logsumexp(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim_,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="366"><highlight class="normal"><sp/><sp/>int64_t<sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim_,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="367"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().tensor();</highlight></codeline>
<codeline lineno="368"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::logsumexp_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="369"><highlight class="normal">}</highlight></codeline>
<codeline lineno="370"><highlight class="normal"></highlight></codeline>
<codeline lineno="371"><highlight class="normal"></highlight><highlight class="comment">//<sp/>\DIM<sp/>REDUCE<sp/>################################################################</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="372"><highlight class="normal"></highlight></codeline>
<codeline lineno="373"><highlight class="normal"></highlight><highlight class="comment">//<sp/>MULTI<sp/>DIM<sp/>REDUCE<sp/>###########################################################</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="374"><highlight class="normal"></highlight></codeline>
<codeline lineno="375"><highlight class="normal"></highlight><highlight class="comment">//<sp/>NB:<sp/>this<sp/>applies<sp/>two<sp/>optimizations:</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="376"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/>1.<sp/>Reducing<sp/>the<sp/>dimensions<sp/>in<sp/>the<sp/>order<sp/>of<sp/>decreasing<sp/>size,<sp/>so<sp/>that<sp/>the</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="377"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>larger<sp/>dimensions<sp/>are<sp/>dealt<sp/>earlier<sp/>and<sp/>we<sp/>can<sp/>work<sp/>with<sp/>less<sp/>elements</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="378"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>overall.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="379"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>E.g.,<sp/>reducing<sp/>tensor<sp/>of<sp/>shape<sp/>[1,<sp/>10,<sp/>200]<sp/>over<sp/>dimemsions<sp/>{0,<sp/>1,<sp/>2}.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="380"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>If<sp/>we<sp/>reduce<sp/>in<sp/>the<sp/>order<sp/>of<sp/>[0,<sp/>1,<sp/>2],<sp/>the<sp/>input<sp/>and<sp/>output</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="381"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>shapes<sp/>of<sp/>iterations<sp/>are:</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="382"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>it<sp/>0:<sp/><sp/>[1,<sp/>10,<sp/>200]<sp/>(2000<sp/>elem)<sp/>=&gt;<sp/>[10,<sp/>200]<sp/>(2000<sp/>elem)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="383"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>it<sp/>1:<sp/><sp/><sp/><sp/><sp/>[10,<sp/>200]<sp/>(2000<sp/>elem)<sp/>=&gt;<sp/><sp/><sp/><sp/><sp/>[200]<sp/>(<sp/>200<sp/>elem)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="384"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>it<sp/>2:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[200]<sp/>(<sp/>200<sp/>elem)<sp/>=&gt;<sp/><sp/><sp/><sp/><sp/>[<sp/><sp/>1]<sp/>(<sp/><sp/><sp/>1<sp/>elem)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="385"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Since<sp/>we<sp/>need<sp/>to<sp/>iterate<sp/>through<sp/>all<sp/>input<sp/>elements<sp/>at<sp/>each</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="386"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>iteration,<sp/>total<sp/>number<sp/>of<sp/>elements<sp/>traversed<sp/>is<sp/>4200.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="387"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>If<sp/>we<sp/>reduce<sp/>in<sp/>the<sp/>order<sp/>of<sp/>[2,<sp/>1,<sp/>0],<sp/>i.e.,<sp/>with<sp/>decreasing</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="388"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>size,<sp/>the<sp/>input<sp/>and<sp/>output<sp/>shapes<sp/>of<sp/>iterations<sp/>are:</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="389"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>it<sp/>0:<sp/><sp/>[1,<sp/>10,<sp/>200]<sp/>(2000<sp/>elem)<sp/>=&gt;<sp/>[1,<sp/>10]<sp/>(10<sp/>elem)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="390"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>it<sp/>1:<sp/><sp/><sp/><sp/><sp/><sp/>[1,<sp/><sp/>10]<sp/>(<sp/><sp/>10<sp/>elem)<sp/>=&gt;<sp/><sp/><sp/><sp/>[<sp/>1]<sp/>(<sp/>1<sp/>elem)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="391"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>it<sp/>2:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[1]<sp/>(<sp/><sp/><sp/>1<sp/>elem)<sp/>=&gt;<sp/><sp/><sp/><sp/>[<sp/>1]<sp/>(<sp/>1<sp/>elem)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="392"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Total<sp/>number<sp/>of<sp/>elements<sp/>traversed<sp/>is<sp/>2011,<sp/>much<sp/>less<sp/>than<sp/>4200.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="393"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/>2.<sp/>Preallocated<sp/>buffer.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="394"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>Utilizing<sp/>the<sp/>`_out`<sp/>variant,<sp/>instead<sp/>of<sp/>allocating<sp/>new<sp/>output<sp/>tensors</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="395"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>at<sp/>each<sp/>iteration,<sp/>we<sp/>can<sp/>use<sp/>a<sp/>preallocated<sp/>buffer.<sp/>Since<sp/>output<sp/>numel</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="396"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>in<sp/>each<sp/>iteration<sp/>is<sp/>decreasing,<sp/>we<sp/>can<sp/>reuse<sp/>the<sp/>buffer<sp/>throughout<sp/>the</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="397"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>loop.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="398"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>Note<sp/>that<sp/>we<sp/>need<sp/>two<sp/>buffers,<sp/>one<sp/>containing<sp/>the<sp/>input,<sp/>i.e.,<sp/>output</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="399"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>from<sp/>the<sp/>previous<sp/>iteration,<sp/>and<sp/>one<sp/>containing<sp/>the<sp/>output<sp/>for<sp/>this</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="400"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>iteration.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="401"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>The<sp/>largest<sp/>output<sp/>size<sp/>is<sp/>the<sp/>output<sp/>size<sp/>of<sp/>the<sp/>first<sp/>iteration.<sp/>After</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="402"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>that<sp/>the<sp/>largest<sp/>size<sp/>we<sp/>need<sp/>is<sp/>the<sp/>output<sp/>size<sp/><sp/>of<sp/>the<sp/>second</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="403"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>iteration.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="404"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>So<sp/>we<sp/>allocate</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="405"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>1.<sp/>a<sp/>region<sp/>of<sp/>size<sp/>`input.numel()<sp/>/<sp/>input.size(reduced_dims[0])`,<sp/>and</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="406"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>2.<sp/>a<sp/>region<sp/>of<sp/>size<sp/>`input.numel()<sp/>/<sp/>(input.size(reduced_dims[0])<sp/>*<sp/>input.size(reduced_dims[1]))`.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="407"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>These<sp/>two<sp/>regions<sp/>are<sp/>allocated<sp/>together<sp/>as<sp/>a<sp/>contiguous<sp/>flattened</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="408"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>buffer<sp/>tensor,<sp/>with<sp/>a<sp/>variable<sp/>`offset`<sp/>indicating<sp/>the<sp/>starting<sp/>position</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="409"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>of<sp/>the<sp/>output<sp/>region<sp/>for<sp/>the<sp/>current<sp/>iteration.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="410"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>E.g.,<sp/>reducing<sp/>tensor<sp/>of<sp/>shape<sp/>[4,<sp/>3,<sp/>2]<sp/>over<sp/>dimemsions<sp/>{0,<sp/>1,<sp/>2}.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="411"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Say<sp/>we<sp/>reduce<sp/>in<sp/>the<sp/>order<sp/>of<sp/>[0,<sp/>1,<sp/>2].</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="412"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>The<sp/>first<sp/>buffer<sp/>with<sp/>has<sp/>size<sp/>`4<sp/>*<sp/>3<sp/>*<sp/>2<sp/>/<sp/>4<sp/>=<sp/>6`.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="413"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>The<sp/>second<sp/>buffer<sp/>with<sp/>has<sp/>size<sp/>`4<sp/>*<sp/>3<sp/>*<sp/>2<sp/>/<sp/>(4<sp/>*<sp/>3)<sp/>=<sp/>2`.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="414"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>So<sp/>we<sp/>allocate<sp/>a<sp/>tensor<sp/>of<sp/>size<sp/>`6<sp/>+<sp/>2<sp/>=<sp/>8`:</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="415"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>buffer:<sp/>[<sp/>_,<sp/>_,<sp/>_,<sp/>_,<sp/>_,<sp/>_,<sp/>_,<sp/>_]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="416"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/>buffer<sp/>region<sp/>1--&gt;^^^^^^^^^^^^^^^^<sp/><sp/>^^^^&lt;--buffer<sp/>region<sp/>2</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="417"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>1st<sp/>iteration:</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="418"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>(before<sp/>reduction)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="419"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>input:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>self<sp/>(or<sp/>input)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="420"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>input<sp/>shape:<sp/><sp/><sp/>[<sp/>4,<sp/>3,<sp/>2]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="421"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>output<sp/>shape:<sp/><sp/>[<sp/>3,<sp/>2]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="422"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>buffer:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[<sp/>_,<sp/>_,<sp/>_,<sp/>_,<sp/>_,<sp/>_,<sp/>_,<sp/>_]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="423"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>offset:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>^--beginning<sp/>of<sp/>1st<sp/>buffer<sp/>region,<sp/>i.e.,<sp/>the</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="424"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>starting<sp/>output<sp/>location<sp/>of<sp/>1st<sp/>iteration.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="425"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>(after<sp/>reduction)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="426"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>buffer:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[<sp/>{output<sp/>of<sp/>1st<sp/>it},<sp/>_,<sp/>_]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="427"><highlight class="normal"></highlight><highlight class="comment">//</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="428"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>2nd<sp/>iteration:</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="429"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>(before<sp/>reduction)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="430"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>input:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>output<sp/>of<sp/>1st<sp/>it</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="431"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>input<sp/>shape:<sp/><sp/><sp/>[<sp/>3,<sp/>2]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="432"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>output<sp/>shape:<sp/><sp/>[<sp/>2]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="433"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>buffer:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[<sp/>{output<sp/>of<sp/>1st<sp/>it},<sp/>_,<sp/>_]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="434"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>offset:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>^--beginning<sp/>of<sp/>2nd</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="435"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>buffer<sp/>region.<sp/>We<sp/>can&apos;t</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="436"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>overwrite<sp/>the<sp/>1st<sp/>buffer</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="437"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>as<sp/>it<sp/>contains<sp/>input<sp/>to</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="438"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>reduction<sp/>of<sp/>this<sp/>it.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="439"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>(after<sp/>reduction)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="440"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>buffer:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[<sp/>{output<sp/>of<sp/>1st<sp/>it},<sp/>{output<sp/>of<sp/>2nd<sp/>it}]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="441"><highlight class="normal"></highlight><highlight class="comment">//</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="442"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>3rd<sp/>iteration:</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="443"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>(before<sp/>reduction)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="444"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>input:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>output<sp/>of<sp/>2nd<sp/>it</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="445"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>input<sp/>shape:<sp/><sp/><sp/>[<sp/>2]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="446"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>output<sp/>shape:<sp/><sp/>[<sp/>1]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="447"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>buffer:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[<sp/>{output<sp/>of<sp/>1st<sp/>it},<sp/>{output<sp/>of<sp/>2nd<sp/>it}]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="448"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>offset:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>^--beginning<sp/>of<sp/>1st<sp/>buffer<sp/>region.<sp/>We<sp/>can</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="449"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>safely<sp/>overwrite<sp/>now.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="450"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>(after<sp/>reduction)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="451"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>buffer:<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[<sp/>{output<sp/>of<sp/>3rd<sp/>it},<sp/>{output<sp/>of<sp/>2nd<sp/>it}]</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="452"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Return<sp/>{output<sp/>of<sp/>3rd<sp/>it}.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="453"><highlight class="normal"></highlight><highlight class="comment">//</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="454"><highlight class="normal"></highlight><highlight class="comment">//<sp/>TODO:<sp/>If<sp/>two<sp/>or<sp/>more<sp/>reduced<sp/>dimensions<sp/>are<sp/>contiguous,<sp/>reduce<sp/>as<sp/>if<sp/>they<sp/>are</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="455"><highlight class="normal"></highlight><highlight class="comment">//<sp/><sp/><sp/><sp/><sp/><sp/><sp/>a<sp/>large<sp/>dimension.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="456"><highlight class="normal"></highlight><highlight class="keyword">template</highlight><highlight class="normal"><sp/>&lt;Tensor<sp/>(reduce_1)(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;,<sp/>int64_t,<sp/>bool),</highlight></codeline>
<codeline lineno="457"><highlight class="normal"><sp/><sp/><sp/><sp/>Tensor&amp;<sp/>(reduce_1_out)(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;,<sp/>int64_t,<sp/>bool)&gt;</highlight></codeline>
<codeline lineno="458"><highlight class="normal"></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor<sp/>reduce_multi_associative(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dims_,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="459"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(dims_.size()<sp/>==<sp/>1)<sp/>{</highlight></codeline>
<codeline lineno="460"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>reduce_1(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dims_[0],<sp/>keepdim);</highlight></codeline>
<codeline lineno="461"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="462"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(dims_.size()<sp/>==<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="463"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">;</highlight></codeline>
<codeline lineno="464"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="465"><highlight class="normal"><sp/><sp/>int64_t<sp/>ndims<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim();</highlight></codeline>
<codeline lineno="466"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>`reduced_numel`<sp/>and<sp/>`reduced_size`<sp/>will<sp/>be<sp/>updated<sp/>in<sp/>the<sp/>loop.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="467"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>Before<sp/>that,<sp/>they<sp/>are<sp/>just<sp/>size<sp/>and<sp/>numel.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="468"><highlight class="normal"><sp/><sp/>int64_t<sp/>reduced_numel<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.numel();</highlight></codeline>
<codeline lineno="469"><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>reduced_size<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.sizes().vec();</highlight></codeline>
<codeline lineno="470"><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>dims<sp/>=<sp/>dims_.vec();</highlight></codeline>
<codeline lineno="471"><highlight class="normal"><sp/><sp/>maybe_wrap_dims(dims,<sp/>ndims);</highlight></codeline>
<codeline lineno="472"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>Sort<sp/>the<sp/>reduced<sp/>dimensions<sp/>so<sp/>that<sp/>we<sp/>reduce<sp/>the<sp/>larger<sp/>dimensions<sp/>first.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="473"><highlight class="normal"><sp/><sp/>std::sort(dims.begin(),<sp/>dims.end(),</highlight></codeline>
<codeline lineno="474"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[&amp;](int64_t<sp/>i,<sp/>int64_t<sp/>j){<sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>reduced_size[i]<sp/>&gt;<sp/>reduced_size[j];<sp/>});</highlight></codeline>
<codeline lineno="475"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>Calculate<sp/>1st<sp/>buffer<sp/>region<sp/>size</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="476"><highlight class="normal"><sp/><sp/>int64_t<sp/>max_reduced_numel<sp/>=<sp/>reduced_numel<sp/>/<sp/>reduced_size[dims[0]];</highlight></codeline>
<codeline lineno="477"><highlight class="normal"><sp/><sp/>int64_t<sp/>buffer_size<sp/>=<sp/>max_reduced_numel<sp/>+<sp/>max_reduced_numel<sp/>/<sp/>reduced_size[dims[1]];</highlight></codeline>
<codeline lineno="478"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>We<sp/>separate<sp/>`buffer`<sp/>into<sp/>two<sp/>regions,<sp/>one<sp/>starting<sp/>at<sp/>0,<sp/>and<sp/>another</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="479"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>starting<sp/>at<sp/>max_reduced_numel.<sp/>These<sp/>two<sp/>regions<sp/>are<sp/>used<sp/>alternatively<sp/>as</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="480"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>the<sp/>output<sp/>of<sp/>a<sp/>`reduce_1`<sp/>along<sp/>a<sp/>particular<sp/>dimension.<sp/>`offset`<sp/>will</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="481"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>indicate<sp/>which<sp/>region<sp/>we<sp/>should<sp/>use<sp/>next.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="482"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>Have<sp/>keepdim=true<sp/>when<sp/>reducing.<sp/>We<sp/>will<sp/>squeeze<sp/>later.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="483"><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>buffer<sp/>=<sp/>at::empty({buffer_size},<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.options());</highlight></codeline>
<codeline lineno="484"><highlight class="normal"><sp/><sp/>int64_t<sp/>offset<sp/>=<sp/>0;</highlight></codeline>
<codeline lineno="485"><highlight class="normal"><sp/><sp/>Tensor<sp/>t<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">;</highlight></codeline>
<codeline lineno="486"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">auto</highlight><highlight class="normal">&amp;<sp/>dim<sp/>:<sp/>dims)<sp/>{</highlight></codeline>
<codeline lineno="487"><highlight class="normal"><sp/><sp/><sp/><sp/>reduced_numel<sp/>/=<sp/>reduced_size[dim];</highlight></codeline>
<codeline lineno="488"><highlight class="normal"><sp/><sp/><sp/><sp/>reduced_size[dim]<sp/>=<sp/>1;</highlight></codeline>
<codeline lineno="489"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>res<sp/>=<sp/>buffer.narrow(0,<sp/>offset,<sp/>reduced_numel).view(reduced_size);</highlight></codeline>
<codeline lineno="490"><highlight class="normal"><sp/><sp/><sp/><sp/>t<sp/>=<sp/>reduce_1_out(res,<sp/>t,<sp/>dim,<sp/></highlight><highlight class="keyword">true</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="491"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>switch<sp/>to<sp/>other<sp/>buffer<sp/>region</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="492"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>this<sp/>alternatively<sp/>changes<sp/>`offset`<sp/>between<sp/>0<sp/>and<sp/>max_reduced_numel</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="493"><highlight class="normal"><sp/><sp/><sp/><sp/>offset<sp/>=<sp/>max_reduced_numel<sp/>-<sp/>offset;</highlight></codeline>
<codeline lineno="494"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="495"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>squeeze<sp/>if<sp/>needed</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="496"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!keepdim)<sp/>{</highlight></codeline>
<codeline lineno="497"><highlight class="normal"><sp/><sp/><sp/><sp/>std::vector&lt;int64_t&gt;<sp/>squeezed_shape;</highlight></codeline>
<codeline lineno="498"><highlight class="normal"><sp/><sp/><sp/><sp/>squeezed_shape.reserve(ndims<sp/>-<sp/>dims.size());</highlight></codeline>
<codeline lineno="499"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>reduce_dims<sp/>=<sp/>dim_list_to_bitset(dims_,<sp/>ndims);</highlight></codeline>
<codeline lineno="500"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(int64_t<sp/>dim<sp/>=<sp/>0;<sp/>dim<sp/>&lt;<sp/>ndims;<sp/>dim++)<sp/>{</highlight></codeline>
<codeline lineno="501"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!reduce_dims[dim])<sp/>{</highlight></codeline>
<codeline lineno="502"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>squeezed_shape.emplace_back(reduced_size[dim]);</highlight></codeline>
<codeline lineno="503"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="504"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="505"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>t.view(squeezed_shape);</highlight></codeline>
<codeline lineno="506"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="507"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>t;</highlight></codeline>
<codeline lineno="508"><highlight class="normal">}</highlight></codeline>
<codeline lineno="509"><highlight class="normal"></highlight></codeline>
<codeline lineno="510"><highlight class="normal"></highlight><highlight class="comment">//<sp/>See<sp/>comments<sp/>above<sp/>reduce_multi_associative<sp/>for<sp/>details.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="511"><highlight class="normal"></highlight><highlight class="keyword">template</highlight><highlight class="normal"><sp/>&lt;Tensor<sp/>(reduce_1)(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;,<sp/>int64_t,<sp/>bool),</highlight></codeline>
<codeline lineno="512"><highlight class="normal"><sp/><sp/><sp/><sp/>Tensor&amp;<sp/>(reduce_1_out)(Tensor&amp;<sp/>result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;,<sp/>int64_t,<sp/>bool)&gt;</highlight></codeline>
<codeline lineno="513"><highlight class="normal"></highlight><highlight class="keyword">inline</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/>reduce_multi_associative_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dims_,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="514"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(dims_.size()<sp/>==<sp/>1)<sp/>{</highlight></codeline>
<codeline lineno="515"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>reduce_1_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dims_[0],<sp/>keepdim);</highlight></codeline>
<codeline lineno="516"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="517"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(dims_.size()<sp/>==<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="518"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>reduce_out<sp/>should<sp/>be<sp/>clone_out<sp/>with<sp/>empty<sp/>dims_</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="519"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result.resize_as_(</highlight><highlight class="keyword">self</highlight><highlight class="normal">).copy_(</highlight><highlight class="keyword">self</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="520"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="521"><highlight class="normal"><sp/><sp/>int64_t<sp/>ndims<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim();</highlight></codeline>
<codeline lineno="522"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>`reduced_numel`<sp/>and<sp/>`reduced_size`<sp/>will<sp/>be<sp/>updated<sp/>in<sp/>the<sp/>loop.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="523"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>Before<sp/>that,<sp/>they<sp/>are<sp/>just<sp/>size<sp/>and<sp/>numel.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="524"><highlight class="normal"><sp/><sp/>int64_t<sp/>reduced_numel<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.numel();</highlight></codeline>
<codeline lineno="525"><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>reduced_size<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.sizes().vec();</highlight></codeline>
<codeline lineno="526"><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>dims<sp/>=<sp/>dims_.vec();</highlight></codeline>
<codeline lineno="527"><highlight class="normal"><sp/><sp/>maybe_wrap_dims(dims,<sp/>ndims);</highlight></codeline>
<codeline lineno="528"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>Sort<sp/>the<sp/>reduced<sp/>dimensions<sp/>so<sp/>that<sp/>we<sp/>reduce<sp/>the<sp/>largest<sp/>dimension<sp/>first.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="529"><highlight class="normal"><sp/><sp/>std::sort(dims.begin(),<sp/>dims.end(),</highlight></codeline>
<codeline lineno="530"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>[&amp;](int64_t<sp/>i,<sp/>int64_t<sp/>j){<sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>reduced_size[i]<sp/>&gt;<sp/>reduced_size[j];<sp/>});</highlight></codeline>
<codeline lineno="531"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>Calculate<sp/>1st<sp/>buffer<sp/>region<sp/>size</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="532"><highlight class="normal"><sp/><sp/>int64_t<sp/>max_reduced_numel<sp/>=<sp/>reduced_numel<sp/>/<sp/>reduced_size[dims[0]];</highlight></codeline>
<codeline lineno="533"><highlight class="normal"><sp/><sp/>int64_t<sp/>buffer_size<sp/>=<sp/>max_reduced_numel<sp/>+<sp/>max_reduced_numel<sp/>/<sp/>reduced_size[dims[1]];</highlight></codeline>
<codeline lineno="534"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>We<sp/>separate<sp/>`buffer`<sp/>into<sp/>two<sp/>regions,<sp/>one<sp/>starting<sp/>at<sp/>0,<sp/>and<sp/>another</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="535"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>starting<sp/>at<sp/>max_reduced_numel.<sp/>These<sp/>two<sp/>regions<sp/>are<sp/>used<sp/>alternatively<sp/>as</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="536"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>the<sp/>output<sp/>of<sp/>a<sp/>`reduce_1`<sp/>along<sp/>a<sp/>particular<sp/>dimension.<sp/>`offset`<sp/>will</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="537"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>indicate<sp/>which<sp/>region<sp/>we<sp/>should<sp/>use<sp/>next.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="538"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>Have<sp/>keepdim=true<sp/>when<sp/>reducing.<sp/>We<sp/>will<sp/>squeeze<sp/>later.</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="539"><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>buffer<sp/>=<sp/>at::empty({buffer_size},<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.options());</highlight></codeline>
<codeline lineno="540"><highlight class="normal"><sp/><sp/>int64_t<sp/>offset<sp/>=<sp/>0;</highlight></codeline>
<codeline lineno="541"><highlight class="normal"><sp/><sp/>Tensor<sp/>t<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">;</highlight></codeline>
<codeline lineno="542"><highlight class="normal"><sp/><sp/>int64_t<sp/>last_reduction<sp/>=<sp/>dims.size()<sp/>-<sp/>1;</highlight></codeline>
<codeline lineno="543"><highlight class="normal"><sp/><sp/>int64_t<sp/>num_reduction<sp/>=<sp/>0;</highlight></codeline>
<codeline lineno="544"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">auto</highlight><highlight class="normal">&amp;<sp/>dim<sp/>:<sp/>dims)<sp/>{</highlight></codeline>
<codeline lineno="545"><highlight class="normal"><sp/><sp/><sp/><sp/>reduced_numel<sp/>/=<sp/>reduced_size[dim];</highlight></codeline>
<codeline lineno="546"><highlight class="normal"><sp/><sp/><sp/><sp/>reduced_size[dim]<sp/>=<sp/>1;</highlight></codeline>
<codeline lineno="547"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>res<sp/>=<sp/>buffer.narrow(0,<sp/>offset,<sp/>reduced_numel).view(reduced_size);</highlight></codeline>
<codeline lineno="548"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(num_reduction<sp/>&lt;<sp/>last_reduction)<sp/>{</highlight></codeline>
<codeline lineno="549"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>t<sp/>=<sp/>reduce_1_out(res,<sp/>t,<sp/>dim,<sp/></highlight><highlight class="keyword">true</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="550"><highlight class="normal"><sp/><sp/><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="551"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>reduce_1_out(result,<sp/>t,<sp/>dim,<sp/></highlight><highlight class="keyword">true</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="552"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="553"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>switch<sp/>to<sp/>other<sp/>buffer<sp/>region</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="554"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>this<sp/>alternatively<sp/>changes<sp/>`offset`<sp/>between<sp/>0<sp/>and<sp/>max_reduced_numel</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="555"><highlight class="normal"><sp/><sp/><sp/><sp/>offset<sp/>=<sp/>max_reduced_numel<sp/>-<sp/>offset;</highlight></codeline>
<codeline lineno="556"><highlight class="normal"><sp/><sp/><sp/><sp/>num_reduction++;</highlight></codeline>
<codeline lineno="557"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="558"><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>squeeze<sp/>if<sp/>needed<sp/>(use<sp/>in-place<sp/>squeeze_)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="559"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!keepdim)<sp/>{</highlight></codeline>
<codeline lineno="560"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>reduce_dims<sp/>=<sp/>dim_list_to_bitset(dims_,<sp/>ndims);</highlight></codeline>
<codeline lineno="561"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(int64_t<sp/>dim<sp/>=<sp/>ndims<sp/>-<sp/>1;<sp/>dim<sp/>&gt;=<sp/>0;<sp/>dim--)<sp/>{</highlight></codeline>
<codeline lineno="562"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(reduce_dims[dim])<sp/>{</highlight></codeline>
<codeline lineno="563"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>result.squeeze_(dim);</highlight></codeline>
<codeline lineno="564"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="565"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="566"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="567"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="568"><highlight class="normal">}</highlight></codeline>
<codeline lineno="569"><highlight class="normal"></highlight></codeline>
<codeline lineno="570"><highlight class="normal">Tensor&amp;<sp/>_sum_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="571"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.is_cuda())<sp/>{</highlight></codeline>
<codeline lineno="572"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_sum_cuda_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="573"><highlight class="normal"><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="574"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>_sum_out_cpu(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="575"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="576"><highlight class="normal">}</highlight></codeline>
<codeline lineno="577"><highlight class="normal"></highlight></codeline>
<codeline lineno="578"><highlight class="normal">Tensor<sp/>_sum(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dims,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="579"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>reduce_multi_associative&lt;_sum,<sp/>_sum_out&gt;(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dims,<sp/>keepdim);</highlight></codeline>
<codeline lineno="580"><highlight class="normal">}</highlight></codeline>
<codeline lineno="581"><highlight class="normal"></highlight></codeline>
<codeline lineno="582"><highlight class="normal">Tensor&amp;<sp/>_sum_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>IntList<sp/>dims,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)</highlight></codeline>
<codeline lineno="583"><highlight class="normal">{</highlight></codeline>
<codeline lineno="584"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>reduce_multi_associative_out&lt;_sum,<sp/>_sum_out&gt;(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dims,<sp/>keepdim);</highlight></codeline>
<codeline lineno="585"><highlight class="normal">}</highlight></codeline>
<codeline lineno="586"><highlight class="normal"></highlight></codeline>
<codeline lineno="587"><highlight class="normal">Tensor<sp/>norm(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>Scalar<sp/>p,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="588"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().tensor();</highlight></codeline>
<codeline lineno="589"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::norm_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>p,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="590"><highlight class="normal">}</highlight></codeline>
<codeline lineno="591"><highlight class="normal"></highlight></codeline>
<codeline lineno="592"><highlight class="normal">Tensor<sp/>&amp;norm_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>Scalar<sp/>p,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="593"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CPU<sp/>||<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CUDA,</highlight></codeline>
<codeline lineno="594"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;norm<sp/>only<sp/>supports<sp/>CPU<sp/>AND<sp/>CUDA<sp/>backend,<sp/>got:<sp/>&quot;</highlight><highlight class="normal">,<sp/>at::toString(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()));</highlight></codeline>
<codeline lineno="595"><highlight class="normal"><sp/><sp/>AT_CHECK(at::isFloatingType(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType()),<sp/></highlight><highlight class="stringliteral">&quot;norm<sp/>only<sp/>supports<sp/>floating-point<sp/>dtypes&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="596"><highlight class="normal"><sp/><sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="597"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(_dimreduce_return_trivial(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>0,<sp/>dim,<sp/>keepdim))<sp/>{</highlight></codeline>
<codeline lineno="598"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="599"><highlight class="normal"><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="600"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_th_norm_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>p,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="601"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="602"><highlight class="normal">}</highlight></codeline>
<codeline lineno="603"><highlight class="normal"></highlight></codeline>
<codeline lineno="604"><highlight class="normal">Tensor<sp/>all(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="605"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().tensor();</highlight></codeline>
<codeline lineno="606"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::all_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="607"><highlight class="normal">}</highlight></codeline>
<codeline lineno="608"><highlight class="normal"></highlight></codeline>
<codeline lineno="609"><highlight class="normal">Tensor<sp/>&amp;all_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="610"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CPU<sp/>||<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CUDA,</highlight></codeline>
<codeline lineno="611"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;all<sp/>only<sp/>supports<sp/>CPU<sp/>AND<sp/>CUDA<sp/>backend,<sp/>got:<sp/>&quot;</highlight><highlight class="normal">,<sp/>at::toString(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()));</highlight></codeline>
<codeline lineno="612"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType()<sp/>==<sp/>at::ScalarType::Byte,<sp/></highlight><highlight class="stringliteral">&quot;all<sp/>only<sp/>supports<sp/>torch.uint8<sp/>dtype&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="613"><highlight class="normal"><sp/><sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="614"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(_dimreduce_return_trivial(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>1,<sp/>dim,<sp/>keepdim))<sp/>{</highlight></codeline>
<codeline lineno="615"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="616"><highlight class="normal"><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="617"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_th_all_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="618"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="619"><highlight class="normal">}</highlight></codeline>
<codeline lineno="620"><highlight class="normal"></highlight></codeline>
<codeline lineno="621"><highlight class="normal">Tensor<sp/>any(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="622"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().tensor();</highlight></codeline>
<codeline lineno="623"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::any_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="624"><highlight class="normal">}</highlight></codeline>
<codeline lineno="625"><highlight class="normal"></highlight></codeline>
<codeline lineno="626"><highlight class="normal">Tensor<sp/>&amp;any_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="627"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CPU<sp/>||<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CUDA,</highlight></codeline>
<codeline lineno="628"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;any<sp/>only<sp/>supports<sp/>CPU<sp/>AND<sp/>CUDA<sp/>backend,<sp/>got:<sp/>&quot;</highlight><highlight class="normal">,<sp/>at::toString(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()));</highlight></codeline>
<codeline lineno="629"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType()<sp/>==<sp/>at::ScalarType::Byte,<sp/></highlight><highlight class="stringliteral">&quot;any<sp/>only<sp/>supports<sp/>torch.uint8<sp/>dtype&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="630"><highlight class="normal"><sp/><sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="631"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(_dimreduce_return_trivial(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>0,<sp/>dim,<sp/>keepdim))<sp/>{</highlight></codeline>
<codeline lineno="632"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="633"><highlight class="normal"><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="634"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_th_any_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>keepdim);</highlight></codeline>
<codeline lineno="635"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="636"><highlight class="normal">}</highlight></codeline>
<codeline lineno="637"><highlight class="normal"></highlight></codeline>
<codeline lineno="638"><highlight class="normal">Tensor<sp/>var(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>unbiased)<sp/>{</highlight></codeline>
<codeline lineno="639"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CPU<sp/>||<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CUDA,</highlight></codeline>
<codeline lineno="640"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;var<sp/>only<sp/>supports<sp/>CPU<sp/>AND<sp/>CUDA<sp/>backend,<sp/>got:<sp/>&quot;</highlight><highlight class="normal">,<sp/>at::toString(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()));</highlight></codeline>
<codeline lineno="641"><highlight class="normal"><sp/><sp/>AT_CHECK(at::isFloatingType(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType()),<sp/></highlight><highlight class="stringliteral">&quot;var<sp/>only<sp/>supports<sp/>floating-point<sp/>dtypes&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="642"><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>trivial_return<sp/>=<sp/>_allreduce_return_trivial(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>std::numeric_limits&lt;double&gt;::quiet_NaN());</highlight></codeline>
<codeline lineno="643"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>trivial_return.has_value()<sp/>?<sp/>trivial_return.value()<sp/>:<sp/>at::_th_var(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>unbiased);</highlight></codeline>
<codeline lineno="644"><highlight class="normal">}</highlight></codeline>
<codeline lineno="645"><highlight class="normal"></highlight></codeline>
<codeline lineno="646"><highlight class="normal">Tensor<sp/>var(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>unbiased,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="647"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().tensor();</highlight></codeline>
<codeline lineno="648"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::var_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>unbiased,<sp/>keepdim);</highlight></codeline>
<codeline lineno="649"><highlight class="normal">}</highlight></codeline>
<codeline lineno="650"><highlight class="normal"></highlight></codeline>
<codeline lineno="651"><highlight class="normal">Tensor<sp/>&amp;var_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>unbiased,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="652"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CPU<sp/>||<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CUDA,</highlight></codeline>
<codeline lineno="653"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;var<sp/>only<sp/>supports<sp/>CPU<sp/>AND<sp/>CUDA<sp/>backend,<sp/>got:<sp/>&quot;</highlight><highlight class="normal">,<sp/>at::toString(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()));</highlight></codeline>
<codeline lineno="654"><highlight class="normal"><sp/><sp/>AT_CHECK(at::isFloatingType(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType()),<sp/></highlight><highlight class="stringliteral">&quot;var<sp/>only<sp/>supports<sp/>floating-point<sp/>dtypes&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="655"><highlight class="normal"><sp/><sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="656"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(_dimreduce_return_trivial(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>std::numeric_limits&lt;double&gt;::quiet_NaN(),<sp/>dim,<sp/>keepdim))<sp/>{</highlight></codeline>
<codeline lineno="657"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="658"><highlight class="normal"><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="659"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_th_var_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>unbiased,<sp/>keepdim);</highlight></codeline>
<codeline lineno="660"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="661"><highlight class="normal">}</highlight></codeline>
<codeline lineno="662"><highlight class="normal"></highlight></codeline>
<codeline lineno="663"><highlight class="normal">Tensor<sp/><ref refid="namespacestd" kindref="compound">std</ref>(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>unbiased)<sp/>{</highlight></codeline>
<codeline lineno="664"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CPU<sp/>||<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CUDA,</highlight></codeline>
<codeline lineno="665"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;std<sp/>only<sp/>supports<sp/>CPU<sp/>AND<sp/>CUDA<sp/>backend,<sp/>got:<sp/>&quot;</highlight><highlight class="normal">,<sp/>at::toString(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()));</highlight></codeline>
<codeline lineno="666"><highlight class="normal"><sp/><sp/>AT_CHECK(at::isFloatingType(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType()),<sp/></highlight><highlight class="stringliteral">&quot;std<sp/>only<sp/>supports<sp/>floating-point<sp/>dtypes&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="667"><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>trivial_return<sp/>=<sp/>_allreduce_return_trivial(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>std::numeric_limits&lt;double&gt;::quiet_NaN());</highlight></codeline>
<codeline lineno="668"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>trivial_return.has_value()<sp/>?<sp/>trivial_return.value()<sp/>:<sp/>at::_th_std(</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>unbiased);</highlight></codeline>
<codeline lineno="669"><highlight class="normal">}</highlight></codeline>
<codeline lineno="670"><highlight class="normal"></highlight></codeline>
<codeline lineno="671"><highlight class="normal">Tensor<sp/><ref refid="namespacestd" kindref="compound">std</ref>(</highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor&amp;<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>unbiased,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="672"><highlight class="normal"><sp/><sp/>Tensor<sp/>result<sp/>=<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().tensor();</highlight></codeline>
<codeline lineno="673"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::native::std_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>unbiased,<sp/>keepdim);</highlight></codeline>
<codeline lineno="674"><highlight class="normal">}</highlight></codeline>
<codeline lineno="675"><highlight class="normal"></highlight></codeline>
<codeline lineno="676"><highlight class="normal">Tensor<sp/>&amp;std_out(Tensor<sp/>&amp;result,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/>Tensor<sp/>&amp;</highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>int64_t<sp/>dim,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>unbiased,<sp/></highlight><highlight class="keywordtype">bool</highlight><highlight class="normal"><sp/>keepdim)<sp/>{</highlight></codeline>
<codeline lineno="677"><highlight class="normal"><sp/><sp/>AT_CHECK(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CPU<sp/>||<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()<sp/>==<sp/>Backend::CUDA,</highlight></codeline>
<codeline lineno="678"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&quot;std<sp/>only<sp/>supports<sp/>CPU<sp/>AND<sp/>CUDA<sp/>backend,<sp/>got:<sp/>&quot;</highlight><highlight class="normal">,<sp/>at::toString(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().backend()));</highlight></codeline>
<codeline lineno="679"><highlight class="normal"><sp/><sp/>AT_CHECK(at::isFloatingType(</highlight><highlight class="keyword">self</highlight><highlight class="normal">.type().scalarType()),<sp/></highlight><highlight class="stringliteral">&quot;std<sp/>only<sp/>supports<sp/>floating-point<sp/>dtypes&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline lineno="680"><highlight class="normal"><sp/><sp/>dim<sp/>=<sp/>maybe_wrap_dim(dim,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">.dim());</highlight></codeline>
<codeline lineno="681"><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(_dimreduce_return_trivial(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>std::numeric_limits&lt;double&gt;::quiet_NaN(),<sp/>dim,<sp/>keepdim))<sp/>{</highlight></codeline>
<codeline lineno="682"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>result;</highlight></codeline>
<codeline lineno="683"><highlight class="normal"><sp/><sp/>}<sp/></highlight><highlight class="keywordflow">else</highlight><highlight class="normal"><sp/>{</highlight></codeline>
<codeline lineno="684"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>at::_th_std_out(result,<sp/></highlight><highlight class="keyword">self</highlight><highlight class="normal">,<sp/>dim,<sp/>unbiased,<sp/>keepdim);</highlight></codeline>
<codeline lineno="685"><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline lineno="686"><highlight class="normal">}</highlight></codeline>
<codeline lineno="687"><highlight class="normal"></highlight></codeline>
<codeline lineno="688"><highlight class="normal">}}<sp/></highlight><highlight class="comment">//<sp/>namespace<sp/>at::native</highlight><highlight class="normal"></highlight></codeline>
    </programlisting>
    <location file="/Users/robkunkle/fork/goodlux/pytorch/aten/src/ATen/native/ReduceOps.cpp"/>
  </compounddef>
</doxygen>
