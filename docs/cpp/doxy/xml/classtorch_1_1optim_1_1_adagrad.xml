<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.14">
  <compounddef id="classtorch_1_1optim_1_1_adagrad" kind="class" language="C++" prot="public">
    <compoundname>torch::optim::Adagrad</compoundname>
    <basecompoundref refid="classtorch_1_1optim_1_1_optimizer" prot="public" virt="non-virtual">torch::optim::Optimizer</basecompoundref>
      <sectiondef kind="friend">
      <memberdef kind="friend" id="classtorch_1_1optim_1_1_adagrad_1ab2f44cbb59a08132f4c843e5225bba0e" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>friend class</type>
        <definition>friend class cereal::access</definition>
        <argsstring></argsstring>
        <name>cereal::access</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="47" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" bodystart="47" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-attrib">
      <memberdef kind="variable" id="classtorch_1_1optim_1_1_adagrad_1a0e625f16db35e1d012e4eadb6b87681c" prot="private" static="no" mutable="no">
        <type><ref refid="structtorch_1_1optim_1_1_adagrad_options" kindref="compound">AdagradOptions</ref></type>
        <definition>AdagradOptions torch::optim::Adagrad::options_</definition>
        <argsstring></argsstring>
        <name>options_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="50" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" bodystart="50" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classtorch_1_1optim_1_1_adagrad_1a8aa3993c3c9be32842ef49c4f7e02c71" prot="private" static="no" mutable="no">
        <type>std::vector&lt; <ref refid="structat_1_1_tensor" kindref="compound">Tensor</ref> &gt;</type>
        <definition>std::vector&lt;Tensor&gt; torch::optim::Adagrad::sum_</definition>
        <argsstring></argsstring>
        <name>sum_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="52" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" bodystart="52" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classtorch_1_1optim_1_1_adagrad_1a05059e40553e4df83609866c14c80e16" prot="private" static="no" mutable="no">
        <type>std::vector&lt; double &gt;</type>
        <definition>std::vector&lt;double&gt; torch::optim::Adagrad::step_</definition>
        <argsstring></argsstring>
        <name>step_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="53" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" bodystart="53" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classtorch_1_1optim_1_1_adagrad_1a75713ae45d1c76464968bb9a069d3a21" prot="public" static="no" const="no" explicit="yes" inline="yes" virt="non-virtual">
        <templateparamlist>
          <param>
            <type>typename ParameterContainer</type>
          </param>
        </templateparamlist>
        <type></type>
        <definition>torch::optim::Adagrad::Adagrad</definition>
        <argsstring>(ParameterContainer &amp;&amp;parameters, const AdagradOptions &amp;options)</argsstring>
        <name>Adagrad</name>
        <param>
          <type>ParameterContainer &amp;&amp;</type>
          <declname>parameters</declname>
        </param>
        <param>
          <type>const <ref refid="structtorch_1_1optim_1_1_adagrad_options" kindref="compound">AdagradOptions</ref> &amp;</type>
          <declname>options</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="28" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" bodystart="28" bodyend="34"/>
      </memberdef>
      <memberdef kind="function" id="classtorch_1_1optim_1_1_adagrad_1aa9fb1176b534af3ee9b00c23e5ef28b2" prot="public" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type>void</type>
        <definition>void torch::optim::Adagrad::step</definition>
        <argsstring>() override</argsstring>
        <name>step</name>
        <reimplements refid="classtorch_1_1optim_1_1_optimizer_1a16ae1552521f10d0ca86377b591747d1">step</reimplements>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Adapted from <ulink url="https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py">https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py</ulink> </para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="36" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/src/optim/adagrad.cpp" bodystart="21" bodyend="41"/>
      </memberdef>
      <memberdef kind="function" id="classtorch_1_1optim_1_1_adagrad_1a2503f72831ff4fc6c39c16e7868974cc" prot="public" static="no" const="yes" explicit="no" inline="no" virt="non-virtual">
        <type>const <ref refid="structtorch_1_1optim_1_1_adagrad_options" kindref="compound">AdagradOptions</ref> &amp;</type>
        <definition>const AdagradOptions &amp; torch::optim::Adagrad::options</definition>
        <argsstring>() const noexcept</argsstring>
        <name>options</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="38" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/src/optim/adagrad.cpp" bodystart="15" bodyend="17"/>
      </memberdef>
      <memberdef kind="function" id="classtorch_1_1optim_1_1_adagrad_1afe8611f7e9da8956732629d9bc1f1e94" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <templateparamlist>
          <param>
            <type>class Archive</type>
          </param>
        </templateparamlist>
        <type>void</type>
        <definition>void torch::optim::Adagrad::serialize</definition>
        <argsstring>(Archive &amp;ar)</argsstring>
        <name>serialize</name>
        <param>
          <type>Archive &amp;</type>
          <declname>ar</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="41" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" bodystart="41" bodyend="44"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-func">
      <memberdef kind="function" id="classtorch_1_1optim_1_1_adagrad_1aba9363fdd51ac87cc9ab16096fed833d" prot="private" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type></type>
        <definition>torch::optim::Adagrad::Adagrad</definition>
        <argsstring>()</argsstring>
        <name>Adagrad</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="48" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" bodystart="48" bodyend="48"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
    </detaileddescription>
    <inheritancegraph>
      <node id="36">
        <label>torch::optim::Adagrad</label>
        <link refid="classtorch_1_1optim_1_1_adagrad"/>
        <childnode refid="37" relation="public-inheritance">
        </childnode>
      </node>
      <node id="38">
        <label>torch::optim::detail::OptimizerBase</label>
        <link refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base"/>
      </node>
      <node id="37">
        <label>torch::optim::Optimizer</label>
        <link refid="classtorch_1_1optim_1_1_optimizer"/>
        <childnode refid="38" relation="public-inheritance">
        </childnode>
      </node>
    </inheritancegraph>
    <collaborationgraph>
      <node id="39">
        <label>torch::optim::Adagrad</label>
        <link refid="classtorch_1_1optim_1_1_adagrad"/>
        <childnode refid="40" relation="public-inheritance">
        </childnode>
      </node>
      <node id="41">
        <label>torch::optim::detail::OptimizerBase</label>
        <link refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base"/>
      </node>
      <node id="40">
        <label>torch::optim::Optimizer</label>
        <link refid="classtorch_1_1optim_1_1_optimizer"/>
        <childnode refid="41" relation="public-inheritance">
        </childnode>
      </node>
    </collaborationgraph>
    <location file="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" line="25" column="1" bodyfile="/Users/robkunkle/fork/goodlux/pytorch/torch/csrc/api/include/torch/optim/adagrad.h" bodystart="25" bodyend="54"/>
    <listofallmembers>
      <member refid="classtorch_1_1optim_1_1_adagrad_1a75713ae45d1c76464968bb9a069d3a21" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>Adagrad</name></member>
      <member refid="classtorch_1_1optim_1_1_adagrad_1aba9363fdd51ac87cc9ab16096fed833d" prot="private" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>Adagrad</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1a35aa4e671083a378e84ead01097854c1" prot="public" virt="virtual"><scope>torch::optim::Adagrad</scope><name>add_parameters</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1a4f0e78d1b05b4f412513e940bb274209" prot="public" virt="virtual"><scope>torch::optim::Adagrad</scope><name>add_parameters</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1a2726328360488171de7ccd2085bcae03" prot="protected" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>buffer_at</name></member>
      <member refid="classtorch_1_1optim_1_1_adagrad_1ab2f44cbb59a08132f4c843e5225bba0e" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>cereal::access</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1ac095094927b128f97db9a5653cb66807" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>OptimizerBase</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1a99c883bd29f37597dac940c420474d38" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>OptimizerBase</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1a08eb2b3fb09eddb7916e790d8e8f4add" prot="protected" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>OptimizerBase</name></member>
      <member refid="classtorch_1_1optim_1_1_adagrad_1a2503f72831ff4fc6c39c16e7868974cc" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>options</name></member>
      <member refid="classtorch_1_1optim_1_1_adagrad_1a0e625f16db35e1d012e4eadb6b87681c" prot="private" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>options_</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1ae3931245065d22fa16cb775c95c33b01" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>ParameterCursor</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1a7fc838f1237cdc236ea515dcb30082e4" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>parameters</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1a1bc457f914c49a04e70a7629488df23f" prot="protected" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>parameters_</name></member>
      <member refid="classtorch_1_1optim_1_1_adagrad_1afe8611f7e9da8956732629d9bc1f1e94" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>serialize</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1af05d9e4c072611745a8dd8f4ff8f6220" prot="public" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>size</name></member>
      <member refid="classtorch_1_1optim_1_1_adagrad_1aa9fb1176b534af3ee9b00c23e5ef28b2" prot="public" virt="virtual"><scope>torch::optim::Adagrad</scope><name>step</name></member>
      <member refid="classtorch_1_1optim_1_1_adagrad_1a05059e40553e4df83609866c14c80e16" prot="private" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>step_</name></member>
      <member refid="classtorch_1_1optim_1_1_adagrad_1a8aa3993c3c9be32842ef49c4f7e02c71" prot="private" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>sum_</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1a60e97f95e8f138600b63fa949bcf2b60" prot="protected" virt="non-virtual"><scope>torch::optim::Adagrad</scope><name>zero_buffers_like</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1abfe0f04c0fa9c13efbf09568985ac834" prot="public" virt="virtual"><scope>torch::optim::Adagrad</scope><name>zero_grad</name></member>
      <member refid="classtorch_1_1optim_1_1detail_1_1_optimizer_base_1aaf3ff0cc2a3a28566bc8c106673af53c" prot="public" virt="virtual"><scope>torch::optim::Adagrad</scope><name>~OptimizerBase</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>
