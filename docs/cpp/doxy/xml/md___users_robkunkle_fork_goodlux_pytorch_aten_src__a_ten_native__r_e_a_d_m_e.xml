<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.14">
  <compounddef id="md___users_robkunkle_fork_goodlux_pytorch_aten_src__a_ten_native__r_e_a_d_m_e" kind="page">
    <compoundname>md__Users_robkunkle_fork_goodlux_pytorch_aten_src_ATen_native_README</compoundname>
    <title>README</title>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
<para>ATen &quot;native&quot; functions are the modern mechanism for adding operators and functions to ATen (they are &quot;native&quot; in contrast to legacy functions, which are bound via TH/THC cwrap metadata). Native functions are declared in <computeroutput>native_functions.yaml</computeroutput> and have implementations defined in one of the <computeroutput>cpp</computeroutput> files in this directory.</para><para>Like all ATen methods/functions, native functions are made available from both ATen&apos;s C++ and Python APIs. In C++, they are made available either as methods on <computeroutput>Tensor</computeroutput> (<computeroutput>t.mymeth()</computeroutput>) and functions in the ATen namespace (<computeroutput>at::myfunc()</computeroutput>). In PyTorch, they are made available as methods on <computeroutput>Variable</computeroutput> or as functions on <computeroutput>torch._C._FunctionBase</computeroutput> (it is the user&apos;s responsibility to re-exporting these functions in a more user-facing module.) At the moment, only functions which ingest <computeroutput>Variable</computeroutput> are made available; to use a function with non-differentiable tensors, wrap your tensors with <computeroutput>Variable</computeroutput> before passing them in.</para><para>The rest of this document describes how to implement an ATen function.</para><para><heading level="2">Registering a function in <computeroutput>native_functions.yaml</computeroutput></heading>
</para><para>Every native function must have an entry in <computeroutput>native_functions.yaml</computeroutput>. The format can be summarized as:</para><para><programlisting><codeline><highlight class="normal">-<sp/>func:<sp/>func_name(ArgType<sp/>arg0[=default],<sp/>ArgType<sp/>arg1[=default],<sp/>...)<sp/>-&gt;<sp/>ReturnType</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>variants:<sp/>function,<sp/>method</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>dispatch:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>CPU:<sp/>func_cpu</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>CUDA:<sp/>func_cuda</highlight></codeline>
</programlisting></para><para>Each component is described in more detail below:</para><para><heading level="3"><computeroutput>func</computeroutput></heading>
</para><para><programlisting><codeline><highlight class="normal">-<sp/>func:<sp/>func_name(ArgType<sp/>arg0[=default],<sp/>ArgType<sp/>arg1[=default],<sp/>...)<sp/>-&gt;<sp/>ReturnType</highlight></codeline>
</programlisting></para><para>The <computeroutput>func</computeroutput> entry is a string describing the name of the function and its type signature.</para><para><bold>Argument types.</bold> These types are permissible as ArgType:</para><para><itemizedlist>
<listitem><para><computeroutput>Tensor</computeroutput>. A <computeroutput>Tensor</computeroutput> argument translates into a C++ argument of type <computeroutput>const Tensor&amp;</computeroutput> (except when the argument is &quot;inplace&quot;; in this case, it is simply <computeroutput>Tensor&amp;</computeroutput>). A trailing <computeroutput>?</computeroutput>, as in <computeroutput>Tensor?</computeroutput>, indicates that the tensor argument is optional and may be omitted by passing an undefined tensor. When a function takes multiple <computeroutput>Tensor</computeroutput> arguments, these tensors are assumed to be the same type (e.g., if one argument is a <computeroutput>FloatTensor</computeroutput>, all other arguments are checked to be <computeroutput>FloatTensor</computeroutput>s.)</para></listitem><listitem><para>Tensors of specific types. At the moment, valid type names are:<itemizedlist>
<listitem><para><computeroutput>IntegerTensor</computeroutput> (a.k.a. <computeroutput>LongTensor</computeroutput>)</para></listitem><listitem><para><computeroutput>BoolTensor</computeroutput> (a.k.a. <computeroutput>ByteTensor</computeroutput>)</para></listitem><listitem><para><computeroutput>IndexTensor</computeroutput> (a.k.a. <computeroutput>IntTensor</computeroutput>) These type names were inherited from TH, and may be renamed soon, so don&apos;t commit them to memory.</para></listitem></itemizedlist>
</para></listitem><listitem><para><computeroutput>TensorList</computeroutput>. A <computeroutput>TensorList</computeroutput> argument translates into a C++ argument of type <computeroutput>ArrayRef&lt;Tensor&gt;</computeroutput> (a.k.a. <computeroutput>TensorList</computeroutput>)</para></listitem><listitem><para><computeroutput>IntList</computeroutput>. <computeroutput>IntList</computeroutput> accepts an optional length specifier, e.g., <computeroutput>IntList[2]</computeroutput>, which has no effect in C++ but extends our Python bindings to accept a bare number, which will be expanded into an appropriately sized list by repeating the number.</para></listitem><listitem><para><computeroutput>int64_t</computeroutput>. There is no <computeroutput>int</computeroutput>; ATen policy is to use <computeroutput>int64_t</computeroutput> in the API anywhere you would have ordinarily passed an <computeroutput>int</computeroutput> or <computeroutput>size_t</computeroutput>.</para></listitem><listitem><para><computeroutput>double</computeroutput>. There is no <computeroutput>float</computeroutput>; ATen policy is to use <computeroutput>double</computeroutput> anywhere you would have used <computeroutput>float</computeroutput>.</para></listitem><listitem><para><computeroutput>bool</computeroutput></para></listitem><listitem><para><computeroutput>Scalar</computeroutput>. <computeroutput>Scalar</computeroutput> supports binding to any numerical types from Python, including integral types, floating point types, and zero dimensional tensors. <computeroutput>int64_t</computeroutput> and <computeroutput>double</computeroutput> can only bind to the corresponding Python numerical types. However, you probably don&apos;t want to use <computeroutput>Scalar</computeroutput>. It&apos;s really used for binding to TH/THC code &quot;real&quot; types where the Python APIs you are binding to are actually different types. <computeroutput>double</computeroutput> and <computeroutput>int64_t</computeroutput> argument types should suffice for most algorithms.</para></listitem><listitem><para><computeroutput>Generator*</computeroutput>, the state for a random number generator,</para></listitem><listitem><para><computeroutput>std::array&lt;bool,N&gt;</computeroutput> (where N is <computeroutput>1-4</computeroutput>). NB: you MUST NOT put a space after the comma, otherwise this argument will not parse correctly. (If you decide to fix this, make sure you fix the argument parser both in ATen and in PyTorch.)</para></listitem><listitem><para><computeroutput>*</computeroutput> is a special sentinel argument, which doesn&apos;t translate into an actual argument, but indicates that in the Python bindings, any subsequent arguments must be specified as keyword arguments (and cannot be provided positionally).</para></listitem></itemizedlist>
</para><para><bold>Return types.</bold> These types are permissible as ReturnType:</para><para><itemizedlist>
<listitem><para><computeroutput>Tensor</computeroutput> and <computeroutput>TensorList</computeroutput>, which translate into the C++ types <computeroutput>Tensor</computeroutput> and <computeroutput>std::vector&lt;Tensor&gt;</computeroutput>, respectively (unless the operation is in-place, in which case the return type is <computeroutput>Tensor&amp;</computeroutput>.</para></listitem><listitem><para>A tuple of any number of <computeroutput>Tensor</computeroutput>, e.g., <computeroutput>(Tensor, Tensor)</computeroutput>, translating into the C++ <computeroutput>std::tuple&lt;Tensor, Tensor&gt;</computeroutput>.</para></listitem></itemizedlist>
</para><para>If you need a type that is not listed in this list, it may be possible to extend ATen&apos;s code generation to support it. ATen&apos;s philosophy on types to support is that it supports only simple, universal types, as well as a handful of fundamental Tensor structures (e.g., <computeroutput>Tensor</computeroutput> and <computeroutput>Generator*</computeroutput>), because these types can be easily ported to any language bound to ATen (in practice, C++ and Python.)</para><para><bold>Argument names.</bold> Argument names are meaningful; downstream binding code may make use of the specific argument name you provide, and a rename of an argument name is considered a BC-breaking change (e.g., you will probably need to update <computeroutput>tools/autograd/derivatives.yaml</computeroutput> at least). In <computeroutput>native_functions.yaml</computeroutput>, if your function (usually functions named with &apos;out&apos; affix) args include the result Tensor, you need to call the argument <computeroutput>Tensor result</computeroutput>. And if there are more than one result Tensors, you need to name the args <computeroutput>Tensor result0, Tensor result1, ...</computeroutput>.</para><para>TODO: Do argument names affect Python keyword arguments?</para><para><bold>Defaults.</bold> Any suffix of arguments can have a default value defined; these default values translate into C++/Python default values which are applied when those positional arguments are not specified.</para><para>Here are the supported default values:</para><para><itemizedlist>
<listitem><para>Numbers (e.g., <computeroutput>0</computeroutput> or <computeroutput>5.0</computeroutput> for <computeroutput>int64_t</computeroutput>, <computeroutput>double</computeroutput> and <computeroutput>IntList</computeroutput> with an explicit length (e.g., <computeroutput>IntList[2]</computeroutput>)<ndash/>in the case of IntList, a number is replicated to fill the length (e.g., <computeroutput>IntList[2] x=2</computeroutput> is equivalent to <computeroutput>IntList[2] x={2,2}</computeroutput>.</para></listitem><listitem><para>Lists of numbers (e.g., <computeroutput>{0, 0}</computeroutput>) for <computeroutput>IntList</computeroutput>.</para></listitem><listitem><para>Booleans (e.g., <computeroutput>true</computeroutput>) for <computeroutput>bool</computeroutput>.</para></listitem><listitem><para>Empty initializer lists (e.g., <computeroutput>{}</computeroutput>) for <computeroutput>Tensor</computeroutput> (this implicitly changes a <computeroutput>Tensor</computeroutput> argument to accept undefined tensors).</para></listitem><listitem><para><computeroutput>nullptr</computeroutput> for pointer types (e.g., <computeroutput>Generator*</computeroutput>)</para></listitem></itemizedlist>
</para><para>The declarations also support the following attributes:</para><para><heading level="3"><computeroutput>variants</computeroutput></heading>
</para><para><programlisting><codeline><highlight class="normal">variants:<sp/>function,<sp/>method</highlight></codeline>
</programlisting></para><para>Controls whether Tensor method (<computeroutput>t.foo()</computeroutput>) or namespace Function (<computeroutput>at::foo()</computeroutput>) is generated as a result of this declaration. If the declaration is a method, you must have an argument <computeroutput>Tensor self</computeroutput> at some position in the method; in the method variant this argument will be elided from the argument list. For example, given the declaration <computeroutput>where(BoolTensor cond, Tensor self, Tensor other)</computeroutput>, this generates the function <computeroutput>at::where(cond, self, other)</computeroutput> and the method <computeroutput>self.where(cond, other)</computeroutput>.</para><para>By default, ATen generates both function and method variants for a native function. Generally, the function variant is always useful; however, you may not wish to generate a method variant. Tensor operations as methods are appropriate for &quot;core&quot; Tensor operations (e.g., add, sub, etc.), but not for more complicated neural network layers (e.g., <computeroutput>conv2d</computeroutput>) and internal functions designed specifically for binding (e.g., <computeroutput>cudnn_convolution</computeroutput>).</para><para><heading level="3"><computeroutput>dispatch</computeroutput></heading>
</para><para><programlisting><codeline><highlight class="normal">dispatch:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>CPU:<sp/>func_cpu</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>CUDA:<sp/>func_cuda</highlight></codeline>
</programlisting></para><para>This specifies the actual name of the function you want to dispatch to, so you can dispatch to different functions depending on whether or not you have CPU or CUDA tensors. Technically, it is also possible to write <computeroutput>dispatch: func_name</computeroutput> to unconditionally dispatch to a native function whose name is different than the name in the public ATen API, but this is generally frowned upon (just name them the same thing!)</para><para><heading level="3"><computeroutput>python_default_init</computeroutput></heading>
</para><para><programlisting><codeline><highlight class="normal">python_default_init:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>argument_name:<sp/>initializing_expression</highlight></codeline>
</programlisting></para><para>A map from argument names to default initializing expressions written in C++. Such default expressions will only be used in Python API (in the C++ API, these arguments are mandatory).</para><para>There are a few situations where you might like to use this functionality:</para><para><itemizedlist>
<listitem><para>You want a default value which is fine in Python but would cause ambiguity in C++. For example, <computeroutput>norm(Tensor self, real p=2, int64_t dim=1)</computeroutput> would cause ambiguity with long tensors in C++. Therefore, we need to make <computeroutput>p=2</computeroutput> a python only default initialization value.</para></listitem><listitem><para>You want a value to default to the same value as another argument (this cannot be expressed in C++ default arguments).</para></listitem></itemizedlist>
</para><para>If you grep for <computeroutput>python_default_init</computeroutput>, you can find examples of this being used; in general, most functions will not need to use this.</para><para><heading level="2">Writing an implementation in C++</heading>
</para><para>Implementations of native functions go in an appropriate C++ file in the <computeroutput>native/</computeroutput> directory (they are organized roughly by topic, but there is no semantic meaning to their organization aside for the <computeroutput>cuda</computeroutput> directory, which is the only place the build system knows how to build <computeroutput>cu</computeroutput> files.) To write a native function, you only need to write a C++ implementation (no header necessary) with a matching signature to the generated header from the ATen metadata. There are many simple native functions; take a look at some of them to see what to do.</para><para>Although, for the most part, writing an ATen function is mostly writing the algorithm you want to implement, there are some less obvious details you should also consider.</para><para><heading level="3">Will your function be automatically differentiable?</heading>
</para><para>If you are writing a pair of functions <computeroutput>foo</computeroutput> and <computeroutput>foo_backward</computeroutput>, with the intent that <computeroutput>foo_backward</computeroutput> implements the derivative of <computeroutput>foo</computeroutput>, then your implementation of <computeroutput>foo</computeroutput> is probably not automatically differentiable: it might make use of functions like <computeroutput>data_ptr()</computeroutput> or it dispatches differently depending on if it&apos;s operating on CPU or CUDA tensors. Once you write these two functions, you will have to write an entry correlating them together in <computeroutput>tools/autograd/derivatives.yaml</computeroutput>.</para><para>However, in some situations, you can write a function in ATen and it will be automatically differentiated! This can be the case if the function implementation only calls other operations which are themselves differentiable. In this case, you don&apos;t have to write an entry in <computeroutput>tools/autograd/derivatives.yaml</computeroutput>.</para><para><heading level="3">Can it handle being passed Variables?</heading>
</para><para>The biggest subtlety of writing an ATen implementation is the fact that <computeroutput>Tensor</computeroutput> is not a &quot;final&quot; class: your implementation may be passed objects which inherit from <computeroutput>Tensor</computeroutput> (in particular, the <computeroutput>Variable</computeroutput> subclass implements automatic differentiation in PyTorch.) This has some direct consequences on valid implementations:</para><para><itemizedlist>
<listitem><para>Never create a <computeroutput>Tensor</computeroutput> directly (e.g., <computeroutput>at::CPU</computeroutput> or <computeroutput>at::CUDA</computeroutput>), as a caller will be expecting to get <computeroutput>Variable</computeroutput>s out if it passes <computeroutput>Variable</computeroutput>. Instead, create tensors from the <computeroutput>type()</computeroutput> of one of the input tensors, e.g., <computeroutput>input.type().tensor()</computeroutput> or <computeroutput>input.type().toScalarType(kByte)</computeroutput> if you need a different scalar type.</para></listitem><listitem><para>If you need to call other ATen functions, be sure to qualify the call with <computeroutput>at::</computeroutput>; don&apos;t call them unqualified (in the <computeroutput>at::native</computeroutput> namespace). Using the qualified name ensures that your invocation gets dispatched to the <computeroutput>Variable</computeroutput> (which may be overridden to behave differently than simply dispatch to <computeroutput>at::native</computeroutput>).</para></listitem></itemizedlist>
</para><para>These are not hard and fast rules: in particular, if you explicitly define a derivative for a function, it will only ever be called with <computeroutput>Tensor</computeroutput> arguments. However, it is considered good style to abide by these rules, since code written in this style is more robust.</para><para>NB: There is one downside to following the <computeroutput>at::</computeroutput> qualification rule, which is that if you know that you will only ever be called with <computeroutput>Tensor</computeroutput>, a direct <computeroutput>at::native</computeroutput> call will be more efficient (as it avoids a dynamic dispatch).</para><para><heading level="3">How to handle broadcasting?</heading>
</para><para>Unlike our legacy TH bindings, ATen native functions do not automatically handle broadcasting; you will have to insert the necessary broadcasting calls yourself.</para><para>When writing broadcasting code, we obey the convention that <computeroutput>op</computeroutput> is broadcasting, while <computeroutput>s_op</computeroutput> (with the <computeroutput>s_</computeroutput> prefix) is not broadcasting. The relationship is best seen by an example of how you would implement broadcasting addition out of non-broadcasting addition:</para><para><programlisting><codeline><highlight class="normal">#include<sp/>&lt;ATen/ExpandUtils.h&gt;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">Tensor<sp/>add(const<sp/>Tensor&amp;<sp/>self,<sp/>const<sp/>Tensor&amp;<sp/>other)<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>Tensor<sp/>b_self,<sp/>b_other;</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>std::tie(b_self,<sp/>b_other)<sp/>=<sp/>expand_outplace(self,<sp/>other,<sp/>&quot;add&quot;);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>return<sp/>s_add(b_self,<sp/>b_other);</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">Tensor<sp/>s_add(const<sp/>Tensor&amp;<sp/>self,<sp/>const<sp/>Tensor&amp;<sp/>other)<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>//<sp/>non-broadcasting<sp/>implementation<sp/>of<sp/>addition</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
</programlisting></para><para>For inplace operations, the convention looks like this:</para><para><programlisting><codeline><highlight class="normal">Tensor&amp;<sp/>add_(Tensor&amp;<sp/>self,<sp/>const<sp/>Tensor&amp;<sp/>other)<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>Tensor<sp/>b_other<sp/>=<sp/>expand_inplace(self,<sp/>other,<sp/>&quot;add_&quot;);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>return<sp/>s_add_(self,<sp/>b_other);</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">Tensor&amp;<sp/>s_add_(Tensor&amp;<sp/>self,<sp/>const<sp/>Tensor&amp;<sp/>other)<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>//<sp/>non-broadcasting<sp/>implementation<sp/>of<sp/>inplace<sp/>addition</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
</programlisting></para><para><heading level="3">Undefined tensor conventions</heading>
</para><para>By default, <computeroutput>Tensor</computeroutput> arguments to ATen functions are always defined, unless you explicitly specified that an undefined tensor was permissible by writing <computeroutput>Tensor?</computeroutput> or <computeroutput>Tensor x={}</computeroutput>.</para><para>The rules for returning undefined Tensors are a bit more subtle, but there is only one case you have to remember:</para><para><itemizedlist>
<listitem><para>If the function in question is a backward function which accepts a <computeroutput>std::array&lt;bool,N&gt; output_mask</computeroutput> argument, you MUST return an undefined <computeroutput>Tensor</computeroutput> at every tuple position <computeroutput>i</computeroutput> for which <computeroutput>output_mask[i]</computeroutput> is false, otherwise</para></listitem><listitem><para>You MUST NOT return an undefined tensor.</para></listitem></itemizedlist>
</para><para>The most common situations where you might be tempted to return undefined tensors are when:</para><para><itemizedlist>
<listitem><para>You have a forward function that may return a buffer if training is enabled, but does not return the buffer in inference mode. In this case, just return an appropriately typed zero-size tensor.</para></listitem><listitem><para>You have a backward function where the gradient for an input is zero. In this case, you are expected to create a zero-filled tensor of appropriate size to return for this input. To get the shape, it may be helpful to take a <computeroutput>TensorGeometry</computeroutput> of the input to use.</para></listitem></itemizedlist>
</para><para><heading level="3">Debugging tips</heading>
</para><para>If you build ATen and get a linker error, that probably means you copy-pasted the C++ definition of your function incorrectly. Double check your <computeroutput>Tensor</computeroutput> arguments, and make sure you wrote <computeroutput>const Tensor&amp;</computeroutput> in your signature. </para>    </detaileddescription>
  </compounddef>
</doxygen>
